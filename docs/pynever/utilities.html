<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pynever.utilities API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pynever.utilities</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import copy

import pynever.strategies.conversion as cv
import pynever.nodes as nodes
import pynever.networks as networks
import torch
import torch.nn.functional as funct
from pynever.tensor import Tensor
import numpy as np
import pynever.pytorch_layers as ptl


def combine_batchnorm1d(linear: ptl.Linear, batchnorm: ptl.BatchNorm1d) -&gt; ptl.Linear:
    &#34;&#34;&#34;
    Utility function to combine a BatchNorm1D node with a Linear node in a corresponding Linear node.
    Parameters
    ----------
    linear : Linear
        Linear to combine.
    batchnorm : BatchNorm1D
        BatchNorm1D to combine.
    Return
    ----------
    Linear
        The Linear resulting from the fusion of the two input nodes.
    &#34;&#34;&#34;

    l_weight = linear.weight
    l_bias = linear.bias
    bn_running_mean = batchnorm.running_mean
    bn_running_var = batchnorm.running_var
    bn_weight = batchnorm.weight
    bn_bias = batchnorm.bias
    bn_eps = batchnorm.eps

    fused_bias = torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps))
    fused_bias = torch.mul(fused_bias, torch.sub(l_bias, bn_running_mean))
    fused_bias = torch.add(fused_bias, bn_bias)

    fused_weight = torch.diag(torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps)))
    fused_weight = torch.matmul(fused_weight, l_weight)

    fused_linear = ptl.Linear(linear.identifier, linear.in_dim, linear.out_dim, linear.in_features, linear.out_features,
                              linear.bias)
    fused_linear.weight = fused_weight
    fused_linear.bias = fused_bias

    return fused_linear


def combine_batchnorm1d_net(network: networks.SequentialNetwork) -&gt; networks.SequentialNetwork:
    &#34;&#34;&#34;
    Utilities function to combine all the FullyConnectedNodes followed by BatchNorm1DNodes in corresponding
    FullyConnectedNodes.
    Parameters
    ----------
    network : SequentialNetwork
        Sequential Network of interest of which we want to combine the nodes.
    Return
    ----------
    SequentialNetwork
        Corresponding Sequential Network with the combined nodes.
    &#34;&#34;&#34;

    py_net = cv.PyTorchConverter().from_neural_network(network)

    modules = [m for m in py_net.pytorch_network.modules()]
    modules = modules[1:]
    num_modules = len(modules)
    current_index = 0

    new_modules = []

    while current_index + 1 &lt; num_modules:

        current_node = modules[current_index]
        next_node = modules[current_index + 1]

        if isinstance(current_node, ptl.Linear) and isinstance(next_node, ptl.BatchNorm1d):
            combined_node = combine_batchnorm1d(current_node, next_node)
            new_modules.append(combined_node)
            current_index = current_index + 1

        elif isinstance(current_node, ptl.Linear):
            new_modules.append(copy.deepcopy(current_node))

        elif isinstance(current_node, nodes.ReLUNode):
            new_modules.append(copy.deepcopy(current_node))

        else:
            raise Exception(&#34;Combine Batchnorm supports only ReLU, Linear and BatchNorm1D layers.&#34;)

        current_index = current_index + 1

    temp_pynet = ptl.Sequential(py_net.pytorch_network.identifier, py_net.pytorch_network.input_id, *new_modules)
    combined_pynet = cv.PyTorchNetwork(py_net.identifier, temp_pynet)
    combined_network = cv.PyTorchConverter().to_neural_network(combined_pynet)

    return combined_network


def generate_targeted_linf_robustness_query(data: Tensor, adv_target: int, bounds: tuple,
                                            num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate a targeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        for i in range(num_classes):

            if i != adv_target:
                f.write(f&#34;(assert (&lt;= (- Y_{i} Y_{adv_target}) 0))\n&#34;)

def generate_untargeted_linf_robustness_query(data: Tensor, target: int, bounds: tuple,
                                              num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate an untargeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        output_query = &#34;(assert (or&#34;
        for i in range(num_classes):

            if i != target:
                output_query += f&#34; (&lt;= (- Y_{target} Y_{i}) 0)&#34;

        output_query += &#34;))&#34;
        f.write(output_query)


def parse_linf_robustness_smtlib(filepath: str) -&gt; (bool, list, int):
    &#34;&#34;&#34;
    Function to extract the parameters of a robustness query from the smtlib file.
    It assume the SMTLIB file is structured as following:

        ; definition of the variables of interest
        (declare-const X_0 Real)
        (declare-const X_1 Real)
        ...
        (declare-const Y_1 Real)
        (declare-const Y_2 Real)
        ...
        ; definition of the constraints
        (assert (&gt;= X_0 eps_0))
        (assert (&lt;= X_0 eps_1))
        ...
        (assert (&lt;= (- Y_0 Y_1) 0))
        ...

    Where the eps_i are Real numbers.

    Parameters
    ----------
    filepath : str
        Filepath to the SMTLIB file.

    Returns
    ----------
    (bool, list, int)
        Tuple of list: the first list contains the values eps_i for each variables as tuples (lower_bound, upper_bound),
        while the int correspond to the desired target for the related data.
    &#34;&#34;&#34;
    targeted = True
    correct_target = -1
    lb = []
    ub = []
    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            line = line.replace(&#39;(&#39;, &#39;( &#39;)
            line = line.replace(&#39;)&#39;, &#39; )&#39;)
            if line[0] == &#39;(&#39;:
                aux = line.split()
                if aux[1] == &#39;assert&#39;:

                    if aux[4] == &#39;(&#39;:
                        if aux[3] == &#39;or&#39;:
                            targeted = False
                            temp = aux[8].split(&#34;_&#34;)
                            correct_target = int(temp[1])
                        else:
                            targeted = True
                            temp = aux[7].split(&#34;_&#34;)
                            correct_target = int(temp[1])

                    else:

                        if aux[3] == &#34;&gt;=&#34;:
                            lb.append(float(aux[5]))
                        else:
                            ub.append(float(aux[5]))

    input_bounds = []
    for i in range(len(lb)):
        input_bounds.append((lb[i], ub[i]))

    return targeted, input_bounds, correct_target


def net_update(network: networks.NeuralNetwork) -&gt; networks.NeuralNetwork:

    if not network.up_to_date:

        for alt_rep in network.alt_rep_cache:

            if alt_rep.up_to_date:
                if isinstance(alt_rep, cv.ONNXNetwork):
                    return cv.ONNXConverter().to_neural_network(alt_rep)
                elif isinstance(alt_rep, cv.PyTorchNetwork):
                    return cv.PyTorchConverter().to_neural_network(alt_rep)
                else:
                    raise NotImplementedError

    else:
        return network


def parse_acas_property(filepath: str) -&gt; ((Tensor, Tensor), (Tensor, Tensor)):

    in_coeff = np.zeros((10, 5))
    in_bias = np.zeros((10, 1))
    out_coeff = []
    out_bias = []
    row_index = 0

    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            if line[0] == &#34;x&#34;:
                splitted_line = line.split(&#34; &#34;)
                var_index = int(splitted_line[0][1])
                if splitted_line[1] == &#34;&gt;=&#34;:
                    in_coeff[row_index, var_index] = -1
                    in_bias[row_index] = -float(splitted_line[2])
                else:
                    in_coeff[row_index, var_index] = 1
                    in_bias[row_index] = float(splitted_line[2])

            else:

                splitted_line = line.split(&#34; &#34;)
                if len(splitted_line) == 3:
                    var_index = int(splitted_line[0][1])
                    temp = np.zeros(5)
                    if splitted_line[1] == &#34;&gt;=&#34;:
                        temp[var_index] = -1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[2]))
                    else:
                        temp[var_index] = 1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[2]))
                else:
                    var_index_1 = int(splitted_line[0][2])
                    var_index_2 = int(splitted_line[1][2])
                    temp = np.zeros(5)
                    if splitted_line[2] == &#34;&gt;=&#34;:
                        temp[var_index_1] = -1
                        temp[var_index_2] = 1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[3]))
                    else:
                        temp[var_index_1] = 1
                        temp[var_index_2] = -1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[3]))

            row_index = row_index + 1

        out_coeff = np.array(out_coeff)
        array_out_bias = np.zeros((len(out_bias), 1))

        for i in range(len(out_bias)):
            array_out_bias[i, 0] = out_bias[i]

        out_bias = array_out_bias

    return (in_coeff, in_bias), (out_coeff, out_bias)


def parse_nnet(filepath: str) -&gt; (list, list, list, list, list, list):

    with open(filepath) as f:

        line = f.readline()
        cnt = 1
        while line[0:2] == &#34;//&#34;:
            line = f.readline()
            cnt += 1
        # numLayers does&#39;t include the input layer!
        numLayers, inputSize, outputSize, maxLayersize = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
        line = f.readline()

        # input layer size, layer1size, layer2size...
        layerSizes = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        symmetric = int(line.strip().split(&#34;,&#34;)[0])

        line = f.readline()
        inputMinimums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        inputMaximums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        means = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        ranges = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        weights = []
        biases = []
        for layernum in range(numLayers):

            previousLayerSize = layerSizes[layernum]
            currentLayerSize = layerSizes[layernum + 1]
            # weights
            weights.append([])
            biases.append([])
            # weights
            for i in range(currentLayerSize):
                line = f.readline()
                aux = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
                weights[layernum].append([])
                for j in range(previousLayerSize):
                    weights[layernum][i].append(aux[j])
            # biases
            for i in range(currentLayerSize):
                line = f.readline()
                x = float(line.strip().split(&#34;,&#34;)[0])
                biases[layernum].append(x)

        numLayers = numLayers
        layerSizes = layerSizes
        inputSize = inputSize
        outputSize = outputSize
        maxLayersize = maxLayersize
        inputMinimums = inputMinimums
        inputMaximums = inputMaximums
        inputMeans = means[:-1]
        inputRanges = ranges[:-1]
        outputMean = means[-1]
        outputRange = ranges[-1]
        weights = weights
        biases = biases

        new_weights = []
        new_biases = []
        for i in range(numLayers):
            weight = np.array(weights[i])
            bias = np.array(biases[i])

            new_weights.append(weight)
            new_biases.append(bias)

        return new_weights, new_biases, inputMeans, inputRanges, outputMean, outputRange


def input_search(net: networks.NeuralNetwork, ref_output: Tensor, start_input: Tensor, max_iter: int, rate: float,
                 threshold: float = 1e-5):

    py_net = cv.PyTorchConverter().from_neural_network(net).pytorch_network
    py_ref_output = torch.from_numpy(ref_output)
    py_start_input = torch.from_numpy(start_input)
    current_input = py_start_input
    current_input.requires_grad = True

    optim = torch.optim.SGD(params=[current_input], lr=rate)

    real_output = py_net(current_input)
    py_ref_output = torch.unsqueeze(py_ref_output, 0)
    real_output = torch.unsqueeze(real_output, 0)
    dist = funct.pairwise_distance(real_output, py_ref_output, p=2)
    iteration = 0

    while dist &gt; threshold and iteration &lt; max_iter:
        #current_input.requires_grad = True
        optim.zero_grad()
        dist.backward()
        optim.step()
        real_output = py_net(current_input)
        real_output = torch.unsqueeze(real_output, 0)
        dist = funct.pairwise_distance(py_ref_output, real_output, p=2)
        print(f&#34;Loss: {dist}&#34;)
        print(f&#34;Current Input: {current_input}&#34;)
        print(f&#34;Current Output: {real_output}&#34;)
        print(f&#34;Grad: {current_input.grad}&#34;)
        print(f&#34;Ref Output: {ref_output}&#34;)
        iteration = iteration + 1

    correct = False
    if dist &lt;= threshold:
        correct = True

    return correct, np.array(current_input), np.array(real_output)


def compute_saliency(net: networks.NeuralNetwork, ref_input: Tensor):

    class BackHook:

        def __init__(self, module: torch.nn.Module, backward=True):
            if backward:
                self.hook = module.register_backward_hook(self.hook_fn)
            else:
                self.hook = module.register_forward_hook(self.hook_fn)
            self.m_input = None
            self.m_output = None

        def hook_fn(self, module, m_input, m_output):
            self.m_input = m_input
            self.m_output = m_output

        def close(self):
            self.hook.remove()

    py_net = cv.PyTorchConverter().from_neural_network(net).pytorch_network

    # We register the hooks on the modules of the networks
    backward_hooks = [BackHook(layer) for layer in py_net.modules()]
    forward_hooks = [BackHook(layer, False) for layer in py_net.modules()]

    ref_input = torch.from_numpy(ref_input)
    ref_input.requires_grad = True
    out = py_net(ref_input)
    i = 0
    print(&#34;FORWARD HOOKS&#34;)
    for m in py_net.modules():
        hook = forward_hooks[i]
        print(m)
        print(&#34;INPUT&#34;)
        print(hook.m_input)
        print(&#34;OUTPUT&#34;)
        print(hook.m_output)
        i = i + 1
    for k in range(len(out)):
        print(f&#34;Variable {k} of output&#34;)
        out = py_net(ref_input)
        out[k].backward(retain_graph=True)
        print(&#34;INPUT GRAD:&#34; + f&#34;{ref_input.grad}&#34;)

        i = 0
        for m in py_net.modules():
            hook = backward_hooks[i]
            print(m)
            print(&#34;INPUT&#34;)
            print(hook.m_input[0])
            print(&#34;OUTPUT&#34;)
            print(hook.m_output[0])
            i = i + 1

    print(out)
    ref_input[0] = ref_input[0] + 10
    out = py_net(ref_input)
    print(out)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pynever.utilities.combine_batchnorm1d"><code class="name flex">
<span>def <span class="ident">combine_batchnorm1d</span></span>(<span>linear: <a title="pynever.pytorch_layers.Linear" href="pytorch_layers.html#pynever.pytorch_layers.Linear">Linear</a>, batchnorm: <a title="pynever.pytorch_layers.BatchNorm1d" href="pytorch_layers.html#pynever.pytorch_layers.BatchNorm1d">BatchNorm1d</a>) ‑> <a title="pynever.pytorch_layers.Linear" href="pytorch_layers.html#pynever.pytorch_layers.Linear">Linear</a></span>
</code></dt>
<dd>
<div class="desc"><p>Utility function to combine a BatchNorm1D node with a Linear node in a corresponding Linear node.
Parameters</p>
<hr>
<dl>
<dt><strong><code>linear</code></strong> :&ensp;<code>Linear</code></dt>
<dd>Linear to combine.</dd>
<dt><strong><code>batchnorm</code></strong> :&ensp;<code>BatchNorm1D</code></dt>
<dd>BatchNorm1D to combine.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Linear
The Linear resulting from the fusion of the two input nodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_batchnorm1d(linear: ptl.Linear, batchnorm: ptl.BatchNorm1d) -&gt; ptl.Linear:
    &#34;&#34;&#34;
    Utility function to combine a BatchNorm1D node with a Linear node in a corresponding Linear node.
    Parameters
    ----------
    linear : Linear
        Linear to combine.
    batchnorm : BatchNorm1D
        BatchNorm1D to combine.
    Return
    ----------
    Linear
        The Linear resulting from the fusion of the two input nodes.
    &#34;&#34;&#34;

    l_weight = linear.weight
    l_bias = linear.bias
    bn_running_mean = batchnorm.running_mean
    bn_running_var = batchnorm.running_var
    bn_weight = batchnorm.weight
    bn_bias = batchnorm.bias
    bn_eps = batchnorm.eps

    fused_bias = torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps))
    fused_bias = torch.mul(fused_bias, torch.sub(l_bias, bn_running_mean))
    fused_bias = torch.add(fused_bias, bn_bias)

    fused_weight = torch.diag(torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps)))
    fused_weight = torch.matmul(fused_weight, l_weight)

    fused_linear = ptl.Linear(linear.identifier, linear.in_dim, linear.out_dim, linear.in_features, linear.out_features,
                              linear.bias)
    fused_linear.weight = fused_weight
    fused_linear.bias = fused_bias

    return fused_linear</code></pre>
</details>
</dd>
<dt id="pynever.utilities.combine_batchnorm1d_net"><code class="name flex">
<span>def <span class="ident">combine_batchnorm1d_net</span></span>(<span>network: <a title="pynever.networks.SequentialNetwork" href="networks.html#pynever.networks.SequentialNetwork">SequentialNetwork</a>) ‑> <a title="pynever.networks.SequentialNetwork" href="networks.html#pynever.networks.SequentialNetwork">SequentialNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Utilities function to combine all the FullyConnectedNodes followed by BatchNorm1DNodes in corresponding
FullyConnectedNodes.
Parameters</p>
<hr>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>SequentialNetwork</code></dt>
<dd>Sequential Network of interest of which we want to combine the nodes.</dd>
</dl>
<h2 id="return">Return</h2>
<p>SequentialNetwork
Corresponding Sequential Network with the combined nodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_batchnorm1d_net(network: networks.SequentialNetwork) -&gt; networks.SequentialNetwork:
    &#34;&#34;&#34;
    Utilities function to combine all the FullyConnectedNodes followed by BatchNorm1DNodes in corresponding
    FullyConnectedNodes.
    Parameters
    ----------
    network : SequentialNetwork
        Sequential Network of interest of which we want to combine the nodes.
    Return
    ----------
    SequentialNetwork
        Corresponding Sequential Network with the combined nodes.
    &#34;&#34;&#34;

    py_net = cv.PyTorchConverter().from_neural_network(network)

    modules = [m for m in py_net.pytorch_network.modules()]
    modules = modules[1:]
    num_modules = len(modules)
    current_index = 0

    new_modules = []

    while current_index + 1 &lt; num_modules:

        current_node = modules[current_index]
        next_node = modules[current_index + 1]

        if isinstance(current_node, ptl.Linear) and isinstance(next_node, ptl.BatchNorm1d):
            combined_node = combine_batchnorm1d(current_node, next_node)
            new_modules.append(combined_node)
            current_index = current_index + 1

        elif isinstance(current_node, ptl.Linear):
            new_modules.append(copy.deepcopy(current_node))

        elif isinstance(current_node, nodes.ReLUNode):
            new_modules.append(copy.deepcopy(current_node))

        else:
            raise Exception(&#34;Combine Batchnorm supports only ReLU, Linear and BatchNorm1D layers.&#34;)

        current_index = current_index + 1

    temp_pynet = ptl.Sequential(py_net.pytorch_network.identifier, py_net.pytorch_network.input_id, *new_modules)
    combined_pynet = cv.PyTorchNetwork(py_net.identifier, temp_pynet)
    combined_network = cv.PyTorchConverter().to_neural_network(combined_pynet)

    return combined_network</code></pre>
</details>
</dd>
<dt id="pynever.utilities.compute_saliency"><code class="name flex">
<span>def <span class="ident">compute_saliency</span></span>(<span>net: <a title="pynever.networks.NeuralNetwork" href="networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, ref_input: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_saliency(net: networks.NeuralNetwork, ref_input: Tensor):

    class BackHook:

        def __init__(self, module: torch.nn.Module, backward=True):
            if backward:
                self.hook = module.register_backward_hook(self.hook_fn)
            else:
                self.hook = module.register_forward_hook(self.hook_fn)
            self.m_input = None
            self.m_output = None

        def hook_fn(self, module, m_input, m_output):
            self.m_input = m_input
            self.m_output = m_output

        def close(self):
            self.hook.remove()

    py_net = cv.PyTorchConverter().from_neural_network(net).pytorch_network

    # We register the hooks on the modules of the networks
    backward_hooks = [BackHook(layer) for layer in py_net.modules()]
    forward_hooks = [BackHook(layer, False) for layer in py_net.modules()]

    ref_input = torch.from_numpy(ref_input)
    ref_input.requires_grad = True
    out = py_net(ref_input)
    i = 0
    print(&#34;FORWARD HOOKS&#34;)
    for m in py_net.modules():
        hook = forward_hooks[i]
        print(m)
        print(&#34;INPUT&#34;)
        print(hook.m_input)
        print(&#34;OUTPUT&#34;)
        print(hook.m_output)
        i = i + 1
    for k in range(len(out)):
        print(f&#34;Variable {k} of output&#34;)
        out = py_net(ref_input)
        out[k].backward(retain_graph=True)
        print(&#34;INPUT GRAD:&#34; + f&#34;{ref_input.grad}&#34;)

        i = 0
        for m in py_net.modules():
            hook = backward_hooks[i]
            print(m)
            print(&#34;INPUT&#34;)
            print(hook.m_input[0])
            print(&#34;OUTPUT&#34;)
            print(hook.m_output[0])
            i = i + 1

    print(out)
    ref_input[0] = ref_input[0] + 10
    out = py_net(ref_input)
    print(out)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.generate_targeted_linf_robustness_query"><code class="name flex">
<span>def <span class="ident">generate_targeted_linf_robustness_query</span></span>(<span>data: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>, adv_target: int, bounds: tuple, num_classes: int, epsilon: float, filepath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to generate a targeted Robustness SMTLIB query and to save it to a SMTLIB file.
The robustness query is of the kind based on the infinity norm.
It assumes that the data and target are from a classification task.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input data of interest.</dd>
<dt><strong><code>adv_target</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired adversarial target for the input data.</dd>
<dt><strong><code>bounds</code></strong> :&ensp;<code>(int, int)</code></dt>
<dd>Bounds for the input data (lower_bound, upper_bound).</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of possible classes.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Perturbation with respect to the infinity norm.</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath for the resulting SMTLIB file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_targeted_linf_robustness_query(data: Tensor, adv_target: int, bounds: tuple,
                                            num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate a targeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        for i in range(num_classes):

            if i != adv_target:
                f.write(f&#34;(assert (&lt;= (- Y_{i} Y_{adv_target}) 0))\n&#34;)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.generate_untargeted_linf_robustness_query"><code class="name flex">
<span>def <span class="ident">generate_untargeted_linf_robustness_query</span></span>(<span>data: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>, target: int, bounds: tuple, num_classes: int, epsilon: float, filepath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to generate an untargeted Robustness SMTLIB query and to save it to a SMTLIB file.
The robustness query is of the kind based on the infinity norm.
It assumes that the data and target are from a classification task.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input data of interest.</dd>
<dt><strong><code>adv_target</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired adversarial target for the input data.</dd>
<dt><strong><code>bounds</code></strong> :&ensp;<code>(int, int)</code></dt>
<dd>Bounds for the input data (lower_bound, upper_bound).</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of possible classes.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Perturbation with respect to the infinity norm.</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath for the resulting SMTLIB file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_untargeted_linf_robustness_query(data: Tensor, target: int, bounds: tuple,
                                              num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate an untargeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        output_query = &#34;(assert (or&#34;
        for i in range(num_classes):

            if i != target:
                output_query += f&#34; (&lt;= (- Y_{target} Y_{i}) 0)&#34;

        output_query += &#34;))&#34;
        f.write(output_query)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.input_search"><code class="name flex">
<span>def <span class="ident">input_search</span></span>(<span>net: <a title="pynever.networks.NeuralNetwork" href="networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, ref_output: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>, start_input: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>, max_iter: int, rate: float, threshold: float = 1e-05)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def input_search(net: networks.NeuralNetwork, ref_output: Tensor, start_input: Tensor, max_iter: int, rate: float,
                 threshold: float = 1e-5):

    py_net = cv.PyTorchConverter().from_neural_network(net).pytorch_network
    py_ref_output = torch.from_numpy(ref_output)
    py_start_input = torch.from_numpy(start_input)
    current_input = py_start_input
    current_input.requires_grad = True

    optim = torch.optim.SGD(params=[current_input], lr=rate)

    real_output = py_net(current_input)
    py_ref_output = torch.unsqueeze(py_ref_output, 0)
    real_output = torch.unsqueeze(real_output, 0)
    dist = funct.pairwise_distance(real_output, py_ref_output, p=2)
    iteration = 0

    while dist &gt; threshold and iteration &lt; max_iter:
        #current_input.requires_grad = True
        optim.zero_grad()
        dist.backward()
        optim.step()
        real_output = py_net(current_input)
        real_output = torch.unsqueeze(real_output, 0)
        dist = funct.pairwise_distance(py_ref_output, real_output, p=2)
        print(f&#34;Loss: {dist}&#34;)
        print(f&#34;Current Input: {current_input}&#34;)
        print(f&#34;Current Output: {real_output}&#34;)
        print(f&#34;Grad: {current_input.grad}&#34;)
        print(f&#34;Ref Output: {ref_output}&#34;)
        iteration = iteration + 1

    correct = False
    if dist &lt;= threshold:
        correct = True

    return correct, np.array(current_input), np.array(real_output)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.net_update"><code class="name flex">
<span>def <span class="ident">net_update</span></span>(<span>network: <a title="pynever.networks.NeuralNetwork" href="networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def net_update(network: networks.NeuralNetwork) -&gt; networks.NeuralNetwork:

    if not network.up_to_date:

        for alt_rep in network.alt_rep_cache:

            if alt_rep.up_to_date:
                if isinstance(alt_rep, cv.ONNXNetwork):
                    return cv.ONNXConverter().to_neural_network(alt_rep)
                elif isinstance(alt_rep, cv.PyTorchNetwork):
                    return cv.PyTorchConverter().to_neural_network(alt_rep)
                else:
                    raise NotImplementedError

    else:
        return network</code></pre>
</details>
</dd>
<dt id="pynever.utilities.parse_acas_property"><code class="name flex">
<span>def <span class="ident">parse_acas_property</span></span>(<span>filepath: str) ‑> ((<class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>, <class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>), (<class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>, <class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>))</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_acas_property(filepath: str) -&gt; ((Tensor, Tensor), (Tensor, Tensor)):

    in_coeff = np.zeros((10, 5))
    in_bias = np.zeros((10, 1))
    out_coeff = []
    out_bias = []
    row_index = 0

    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            if line[0] == &#34;x&#34;:
                splitted_line = line.split(&#34; &#34;)
                var_index = int(splitted_line[0][1])
                if splitted_line[1] == &#34;&gt;=&#34;:
                    in_coeff[row_index, var_index] = -1
                    in_bias[row_index] = -float(splitted_line[2])
                else:
                    in_coeff[row_index, var_index] = 1
                    in_bias[row_index] = float(splitted_line[2])

            else:

                splitted_line = line.split(&#34; &#34;)
                if len(splitted_line) == 3:
                    var_index = int(splitted_line[0][1])
                    temp = np.zeros(5)
                    if splitted_line[1] == &#34;&gt;=&#34;:
                        temp[var_index] = -1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[2]))
                    else:
                        temp[var_index] = 1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[2]))
                else:
                    var_index_1 = int(splitted_line[0][2])
                    var_index_2 = int(splitted_line[1][2])
                    temp = np.zeros(5)
                    if splitted_line[2] == &#34;&gt;=&#34;:
                        temp[var_index_1] = -1
                        temp[var_index_2] = 1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[3]))
                    else:
                        temp[var_index_1] = 1
                        temp[var_index_2] = -1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[3]))

            row_index = row_index + 1

        out_coeff = np.array(out_coeff)
        array_out_bias = np.zeros((len(out_bias), 1))

        for i in range(len(out_bias)):
            array_out_bias[i, 0] = out_bias[i]

        out_bias = array_out_bias

    return (in_coeff, in_bias), (out_coeff, out_bias)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.parse_linf_robustness_smtlib"><code class="name flex">
<span>def <span class="ident">parse_linf_robustness_smtlib</span></span>(<span>filepath: str) ‑> (<class 'bool'>, <class 'list'>, <class 'int'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to extract the parameters of a robustness query from the smtlib file.
It assume the SMTLIB file is structured as following:</p>
<pre><code>; definition of the variables of interest
(declare-const X_0 Real)
(declare-const X_1 Real)
...
(declare-const Y_1 Real)
(declare-const Y_2 Real)
...
; definition of the constraints
(assert (&gt;= X_0 eps_0))
(assert (&lt;= X_0 eps_1))
...
(assert (&lt;= (- Y_0 Y_1) 0))
...
</code></pre>
<p>Where the eps_i are Real numbers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath to the SMTLIB file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(bool, list, int)
Tuple of list: the first list contains the values eps_i for each variables as tuples (lower_bound, upper_bound),
while the int correspond to the desired target for the related data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_linf_robustness_smtlib(filepath: str) -&gt; (bool, list, int):
    &#34;&#34;&#34;
    Function to extract the parameters of a robustness query from the smtlib file.
    It assume the SMTLIB file is structured as following:

        ; definition of the variables of interest
        (declare-const X_0 Real)
        (declare-const X_1 Real)
        ...
        (declare-const Y_1 Real)
        (declare-const Y_2 Real)
        ...
        ; definition of the constraints
        (assert (&gt;= X_0 eps_0))
        (assert (&lt;= X_0 eps_1))
        ...
        (assert (&lt;= (- Y_0 Y_1) 0))
        ...

    Where the eps_i are Real numbers.

    Parameters
    ----------
    filepath : str
        Filepath to the SMTLIB file.

    Returns
    ----------
    (bool, list, int)
        Tuple of list: the first list contains the values eps_i for each variables as tuples (lower_bound, upper_bound),
        while the int correspond to the desired target for the related data.
    &#34;&#34;&#34;
    targeted = True
    correct_target = -1
    lb = []
    ub = []
    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            line = line.replace(&#39;(&#39;, &#39;( &#39;)
            line = line.replace(&#39;)&#39;, &#39; )&#39;)
            if line[0] == &#39;(&#39;:
                aux = line.split()
                if aux[1] == &#39;assert&#39;:

                    if aux[4] == &#39;(&#39;:
                        if aux[3] == &#39;or&#39;:
                            targeted = False
                            temp = aux[8].split(&#34;_&#34;)
                            correct_target = int(temp[1])
                        else:
                            targeted = True
                            temp = aux[7].split(&#34;_&#34;)
                            correct_target = int(temp[1])

                    else:

                        if aux[3] == &#34;&gt;=&#34;:
                            lb.append(float(aux[5]))
                        else:
                            ub.append(float(aux[5]))

    input_bounds = []
    for i in range(len(lb)):
        input_bounds.append((lb[i], ub[i]))

    return targeted, input_bounds, correct_target</code></pre>
</details>
</dd>
<dt id="pynever.utilities.parse_nnet"><code class="name flex">
<span>def <span class="ident">parse_nnet</span></span>(<span>filepath: str) ‑> (<class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_nnet(filepath: str) -&gt; (list, list, list, list, list, list):

    with open(filepath) as f:

        line = f.readline()
        cnt = 1
        while line[0:2] == &#34;//&#34;:
            line = f.readline()
            cnt += 1
        # numLayers does&#39;t include the input layer!
        numLayers, inputSize, outputSize, maxLayersize = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
        line = f.readline()

        # input layer size, layer1size, layer2size...
        layerSizes = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        symmetric = int(line.strip().split(&#34;,&#34;)[0])

        line = f.readline()
        inputMinimums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        inputMaximums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        means = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        ranges = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        weights = []
        biases = []
        for layernum in range(numLayers):

            previousLayerSize = layerSizes[layernum]
            currentLayerSize = layerSizes[layernum + 1]
            # weights
            weights.append([])
            biases.append([])
            # weights
            for i in range(currentLayerSize):
                line = f.readline()
                aux = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
                weights[layernum].append([])
                for j in range(previousLayerSize):
                    weights[layernum][i].append(aux[j])
            # biases
            for i in range(currentLayerSize):
                line = f.readline()
                x = float(line.strip().split(&#34;,&#34;)[0])
                biases[layernum].append(x)

        numLayers = numLayers
        layerSizes = layerSizes
        inputSize = inputSize
        outputSize = outputSize
        maxLayersize = maxLayersize
        inputMinimums = inputMinimums
        inputMaximums = inputMaximums
        inputMeans = means[:-1]
        inputRanges = ranges[:-1]
        outputMean = means[-1]
        outputRange = ranges[-1]
        weights = weights
        biases = biases

        new_weights = []
        new_biases = []
        for i in range(numLayers):
            weight = np.array(weights[i])
            bias = np.array(biases[i])

            new_weights.append(weight)
            new_biases.append(bias)

        return new_weights, new_biases, inputMeans, inputRanges, outputMean, outputRange</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pynever" href="index.html">pynever</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pynever.utilities.combine_batchnorm1d" href="#pynever.utilities.combine_batchnorm1d">combine_batchnorm1d</a></code></li>
<li><code><a title="pynever.utilities.combine_batchnorm1d_net" href="#pynever.utilities.combine_batchnorm1d_net">combine_batchnorm1d_net</a></code></li>
<li><code><a title="pynever.utilities.compute_saliency" href="#pynever.utilities.compute_saliency">compute_saliency</a></code></li>
<li><code><a title="pynever.utilities.generate_targeted_linf_robustness_query" href="#pynever.utilities.generate_targeted_linf_robustness_query">generate_targeted_linf_robustness_query</a></code></li>
<li><code><a title="pynever.utilities.generate_untargeted_linf_robustness_query" href="#pynever.utilities.generate_untargeted_linf_robustness_query">generate_untargeted_linf_robustness_query</a></code></li>
<li><code><a title="pynever.utilities.input_search" href="#pynever.utilities.input_search">input_search</a></code></li>
<li><code><a title="pynever.utilities.net_update" href="#pynever.utilities.net_update">net_update</a></code></li>
<li><code><a title="pynever.utilities.parse_acas_property" href="#pynever.utilities.parse_acas_property">parse_acas_property</a></code></li>
<li><code><a title="pynever.utilities.parse_linf_robustness_smtlib" href="#pynever.utilities.parse_linf_robustness_smtlib">parse_linf_robustness_smtlib</a></code></li>
<li><code><a title="pynever.utilities.parse_nnet" href="#pynever.utilities.parse_nnet">parse_nnet</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>