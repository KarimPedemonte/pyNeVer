<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.1.0"/>
    <title>pynever.strategies.pruning API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../strategies.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;pynever.strategies</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#PruningStrategy">PruningStrategy</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#PruningStrategy.prune">prune</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#WPTransform">WPTransform</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#WPTransform.__init__">WPTransform</a>
                        </li>
                        <li>
                                <a class="variable" href="#WPTransform.l1_decay">l1_decay</a>
                        </li>
                        <li>
                                <a class="variable" href="#WPTransform.fine_tuning">fine_tuning</a>
                        </li>
                        <li>
                                <a class="variable" href="#WPTransform.cuda">cuda</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#NSTransform">NSTransform</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#NSTransform.__init__">NSTransform</a>
                        </li>
                        <li>
                                <a class="variable" href="#NSTransform.batchnorm_decay">batchnorm_decay</a>
                        </li>
                        <li>
                                <a class="variable" href="#NSTransform.fine_tuning">fine_tuning</a>
                        </li>
                        <li>
                                <a class="variable" href="#NSTransform.cuda">cuda</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#WeightPruning">WeightPruning</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#WeightPruning.__init__">WeightPruning</a>
                        </li>
                        <li>
                                <a class="variable" href="#WeightPruning.sparsity_rate">sparsity_rate</a>
                        </li>
                        <li>
                                <a class="variable" href="#WeightPruning.training_strategy">training_strategy</a>
                        </li>
                        <li>
                                <a class="variable" href="#WeightPruning.pre_training">pre_training</a>
                        </li>
                        <li>
                                <a class="function" href="#WeightPruning.prune">prune</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#NetworkSlimming">NetworkSlimming</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#NetworkSlimming.__init__">NetworkSlimming</a>
                        </li>
                        <li>
                                <a class="variable" href="#NetworkSlimming.sparsity_rate">sparsity_rate</a>
                        </li>
                        <li>
                                <a class="variable" href="#NetworkSlimming.training_strategy">training_strategy</a>
                        </li>
                        <li>
                                <a class="variable" href="#NetworkSlimming.pre_training">pre_training</a>
                        </li>
                        <li>
                                <a class="function" href="#NetworkSlimming.prune">prune</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../pynever.html">pynever</a><wbr>.<a href="./../strategies.html">strategies</a><wbr>.pruning    </h1>

                
                        <input id="mod-pruning-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-pruning-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="kn">import</span> <span class="nn">abc</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="kn">import</span> <span class="nn">math</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="kn">import</span> <span class="nn">torch</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span> <span class="nn">pynever.datasets</span> <span class="k">as</span> <span class="nn">datasets</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">import</span> <span class="nn">pynever.networks</span> <span class="k">as</span> <span class="nn">networks</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="kn">import</span> <span class="nn">pynever.pytorch_layers</span> <span class="k">as</span> <span class="nn">ptl</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">import</span> <span class="nn">pynever.strategies.conversion</span> <span class="k">as</span> <span class="nn">cv</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="kn">import</span> <span class="nn">pynever.strategies.training</span> <span class="k">as</span> <span class="nn">training</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="k">class</span> <span class="nc">PruningStrategy</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="sd">    An abstract class used to represent a Pruning Strategy.</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="sd">    Methods</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="sd">    ----------</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="sd">    prune(NeuralNetwork, Dataset)</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="sd">        Prune the neural network of interest using a pruning strategy determined in the concrete children.</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="sd">        Prune the neural network of interest using a pruning strategy determined in the concrete children.</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a><span class="sd">        Parameters</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a><span class="sd">        ----------</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a><span class="sd">        dataset: Dataset</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a><span class="sd">        Returns</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a><span class="sd">        ----------</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>        <span class="k">pass</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="k">class</span> <span class="nc">WPTransform</span><span class="p">:</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="sd">    Callable to pass to the training strategy used in the pruning in order to optimize it for the pruning.</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="sd">    Attributes</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">    ----------</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="sd">    l1_decay : float</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">        Coefficient of the L1 norm regularizer used on the weights of the linear layers of the network</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a><span class="sd">        to push the weights to near-to-zero values.</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">    fine_tuning : bool</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="sd">        If True the weight with value zero are not updated by the optimizer step, otherwise the L1 regularizer</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">        is used.</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">    cuda : bool, Optional</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="sd">        It should be the same of the training strategy receiving the callable as network transform</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">l1_decay</span> <span class="o">=</span> <span class="n">l1_decay</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">cuda</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>                    <span class="n">weight_copy</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>                        <span class="n">mask</span> <span class="o">=</span> <span class="n">weight_copy</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>                    <span class="k">else</span><span class="p">:</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>                        <span class="n">mask</span> <span class="o">=</span> <span class="n">weight_copy</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>                    <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>                    <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_decay</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a><span class="k">class</span> <span class="nc">NSTransform</span><span class="p">:</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a><span class="sd">    Callable to pass to the training strategy used in the pruning in order to optimize it for the pruning.</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a><span class="sd">    Attributes</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a><span class="sd">    ----------</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a><span class="sd">    batchnorm_decay : float</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="sd">        Coefficient of the L1 norm regularizer used on the batchnorm layers of the network</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a><span class="sd">        to push the weights to near-to-zero values.</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="sd">    fine_tuning : bool</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="sd">        If True the L1 regularizer is not applied to the network.</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a><span class="sd">    cuda : bool, Optional</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a><span class="sd">        It should be the same of the training strategy receiving the callable as network transform</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchnorm_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm_decay</span> <span class="o">=</span> <span class="n">batchnorm_decay</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">cuda</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>                    <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchnorm_decay</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a><span class="k">class</span> <span class="nc">WeightPruning</span><span class="p">(</span><span class="n">PruningStrategy</span><span class="p">):</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a><span class="sd">    A concrete class used to represent the weight pruning strategy.</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a><span class="sd">    This kind of pruning select the least important weights of the neural network</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a><span class="sd">    of interest and set them to 0. It assume vectorial input for the linear layers.</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a><span class="sd">    We refer to https://arxiv.org/abs/1506.02626 for theoretical details on the strategy.</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a><span class="sd">    Attributes</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a><span class="sd">    ----------</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a><span class="sd">    sparsity_rate : float</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a><span class="sd">        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a><span class="sd">    training_strategy : PytorchTraining</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a><span class="sd">        The training strategy to use for pre-training and/or fine-tuning. NB: Its network_transform parameter must</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a><span class="sd">        be of the class WPTransform.</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="sd">    pre_training : bool</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">        Flag to indicate if the network need to be pre-trained.</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="sd">    Methods</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="sd">    ----------</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="sd">    prune(NeuralNetwork, Dataset)</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Weight Pruning.</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">training_strategy</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">PytorchTraining</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>                 <span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span> <span class="o">=</span> <span class="n">sparsity_rate</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="o">=</span> <span class="n">training_strategy</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="p">,</span> <span class="n">WPTransform</span><span class="p">):</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The network_transform attribute of the training_strategy should be of&quot;</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>                            <span class="s2">&quot; the class WPTransform&quot;</span><span class="p">)</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span> <span class="o">=</span> <span class="n">pre_training</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pre_training</span><span class="p">:</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;If pre_training is True then training_strategy must not be None&quot;</span><span class="p">)</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Weight Pruning.</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="sd">        Parameters</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a><span class="sd">        ----------</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a><span class="sd">        dataset : Dataset</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a><span class="sd">        Returns</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a><span class="sd">        ----------</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span><span class="p">:</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>            <span class="n">fine_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>        <span class="n">pytorch_converter</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchConverter</span><span class="p">()</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="n">pytorch_converter</span><span class="o">.</span><span class="n">from_neural_network</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pruning</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">up_to_date</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>        <span class="k">return</span> <span class="n">network</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>    <span class="k">def</span> <span class="nf">__pruning</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a><span class="sd">        Procedure for the pruning of the weights of the PyTorchNetwork passed as an argument.</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a><span class="sd">        Parameters</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a><span class="sd">        ----------</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a><span class="sd">        net : PyTorchNetwork</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a><span class="sd">            The PyTorchNetwork to prune.</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a><span class="sd">        Returns</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a><span class="sd">        ----------</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a><span class="sd">        PyTorchNetwork</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a><span class="sd">            The pruned PyTorchNetwork.</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>        <span class="c1"># We transfer the internal pytorch model to the CPU for the pruning procedure.</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>        <span class="c1"># We compute the number of weights in the network</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>        <span class="n">num_weights</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>                <span class="n">num_weights</span> <span class="o">+=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>        <span class="c1"># We copy all the absolute values of the weights in a new tensor and we sort in ascending order</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_weights</span><span class="p">)</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>        <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>                <span class="n">size</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>                <span class="n">weights</span><span class="p">[</span><span class="n">index</span><span class="p">:(</span><span class="n">index</span> <span class="o">+</span> <span class="n">size</span><span class="p">)]</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>                <span class="n">index</span> <span class="o">+=</span> <span class="n">size</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>        <span class="n">ordered_weights</span><span class="p">,</span> <span class="n">ordered_indexes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>        <span class="c1"># We determine the number of weights we need to set to 0, given the sparsity rate.</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>        <span class="n">threshold_index</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">num_weights</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span><span class="p">)</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>        <span class="c1"># We select the weight absolute value we will use as threshold value given the threshold index.</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>        <span class="n">threshold_value</span> <span class="o">=</span> <span class="n">ordered_weights</span><span class="p">[</span><span class="n">threshold_index</span><span class="p">]</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>        <span class="c1"># We set all the weights of the different layers to 0 if they are less or equal than the threshold value</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>        <span class="c1"># (in absolute value)</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>                <span class="c1"># The values of the mask are 0 when the corresponding weight is less then the threshold_value (in</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>                <span class="c1"># absolute value), otherwise they are 1</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>                <span class="n">mask</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">threshold_value</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>        <span class="k">return</span> <span class="n">net</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a><span class="k">class</span> <span class="nc">NetworkSlimming</span><span class="p">(</span><span class="n">PruningStrategy</span><span class="p">):</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a><span class="sd">    A concrete class used to represent the network slimming pruning strategy.</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a><span class="sd">    This kind of pruning select the least important neurons of the neural network</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a><span class="sd">    of interest and eliminates them. It needs a batch normalization layer following each layer</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a><span class="sd">    which should be pruned. We assume that the activation function is always applied after the batch</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a><span class="sd">    normalization layer. It support only networks with linear and batchnorm layers with vectorial inputs</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a><span class="sd">    We refer to https://arxiv.org/abs/1708.06519 for theoretical details on the strategy.</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a><span class="sd">    Attributes</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a><span class="sd">    ----------</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a><span class="sd">    sparsity_rate : float</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a><span class="sd">        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a><span class="sd">    training_strategy : PytorchTraining</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a><span class="sd">        The training strategy to use for pre-training and/or fine-tuning. NB: Its network_transform parameter must</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a><span class="sd">        be of the class NSTransform.</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a><span class="sd">    pre_training : bool</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a><span class="sd">        Flag to indicate if the network need to be pre-trained.</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a><span class="sd">    Methods</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a><span class="sd">    ----------</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a><span class="sd">    prune(NeuralNetwork, Dataset)</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Network Slimming.</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">training_strategy</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">PytorchTraining</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>                 <span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span> <span class="o">=</span> <span class="n">sparsity_rate</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="o">=</span> <span class="n">training_strategy</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="p">,</span> <span class="n">NSTransform</span><span class="p">):</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The network_transform attribute of the training_strategy should be of&quot;</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a>                            <span class="s2">&quot; the class NSTransform&quot;</span><span class="p">)</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span> <span class="o">=</span> <span class="n">pre_training</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pre_training</span><span class="p">:</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;If pre_training is True then training_strategy must not be None&quot;</span><span class="p">)</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Network Slimming.</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a><span class="sd">        Parameters</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a><span class="sd">        ----------</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a><span class="sd">        dataset: Dataset</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a><span class="sd">        Returns</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a><span class="sd">        ----------</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span><span class="p">:</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>            <span class="n">fine_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a>        <span class="n">pytorch_converter</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchConverter</span><span class="p">()</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="n">pytorch_converter</span><span class="o">.</span><span class="n">from_neural_network</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pruning</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">up_to_date</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>        <span class="k">return</span> <span class="n">network</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a>    <span class="k">def</span> <span class="nf">__pruning</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a><span class="sd">        Procedure for the pruning of the neurons of the PyTorchNetwork passed as an argument.</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a><span class="sd">        Parameters</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a><span class="sd">        ----------</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a><span class="sd">        net : PyTorchNetwork</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a><span class="sd">            The PyTorchNetwork to prune.</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a><span class="sd">        Returns</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a><span class="sd">        ----------</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a><span class="sd">        PyTorchNetwork</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a><span class="sd">            The PyTorchNetwork resulting from the application of the pure pruning procedure.</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a>        <span class="c1"># We transfer the internal pytorch model to the CPU for the pruning procedure.</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>        <span class="c1"># We compute the total number of weights in the batch normalization layers (which, for fully connected networks,</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>        <span class="c1"># is equal to the number of neurons in the corresponding fully-connected layer).</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>        <span class="n">num_bn_weights</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>                <span class="n">num_bn_weights</span> <span class="o">+=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>        <span class="c1"># We copy all the absolute values of the batch norm weights in a new tensor and we sort in ascending order</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>        <span class="n">bn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_bn_weights</span><span class="p">)</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>        <span class="n">bn_weights_index</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>                <span class="n">size</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>                <span class="n">bn_weights</span><span class="p">[</span><span class="n">bn_weights_index</span><span class="p">:(</span><span class="n">bn_weights_index</span> <span class="o">+</span> <span class="n">size</span><span class="p">)]</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>                <span class="n">bn_weights_index</span> <span class="o">+=</span> <span class="n">size</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>        <span class="n">ordered_bn_weights</span><span class="p">,</span> <span class="n">ordered_bn_indexes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">bn_weights</span><span class="p">)</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>        <span class="c1"># We determine the number of neurons we need to remove, given the sparsity rate.</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>        <span class="n">threshold_index</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">num_bn_weights</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span><span class="p">)</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>        <span class="c1"># We select the batch norm weight absolute value we will use as threshold value given the threshold index.</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>        <span class="n">threshold_value</span> <span class="o">=</span> <span class="n">ordered_bn_weights</span><span class="p">[</span><span class="n">threshold_index</span><span class="p">]</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>        <span class="c1"># We now need to create a new network with the correct number of neurons in the different layers.</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>        <span class="c1"># To do so we assume that in the network after a linear layer there is always a batch norm layer.</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>        <span class="n">new_layers</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>        <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>        <span class="n">old_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">()]</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>        <span class="n">orig_seq</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>        <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">old_layers</span><span class="p">)</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>                <span class="c1"># In this case we are considering the last layer of the network (which we assume to be a linear layer),</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a>                <span class="c1"># therefore the number of output of the new layer will be equal to the one of the old layer.</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>                <span class="n">previous_nonzero_indexes</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>                <span class="c1"># If the old linear layer had bias then also the new linear layer has them.</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>                <span class="k">if</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>                <span class="c1"># The number of input features for the new linear layer is equal to the number of non-zero elements in</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>                <span class="c1"># the mask of the previous layer.</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>                <span class="n">num_in_features</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>                <span class="c1"># We create the new linear layer with the correct architecture.</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_in_features</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>                <span class="n">new_linear_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>                                              <span class="n">num_in_features</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">)</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons.</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">previous_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>                <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>                    <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a>                <span class="c1"># We save the new linear layer.</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_linear_layer</span><span class="p">)</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>                <span class="c1"># If the layer old_layers[i] is the first linear layer then the previous layer mask corrspond to the</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>                <span class="c1"># complete input.</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a>                <span class="k">if</span> <span class="n">previous_layer_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>                    <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>                <span class="c1"># We compute the mask corresponding to the batch normalization layer.</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a>                <span class="n">layer_mask</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">threshold_value</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a>                <span class="n">new_neuron_number</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a>                <span class="c1"># We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a>                <span class="n">current_nonzero_indexes</span> <span class="o">=</span> <span class="n">layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a>                <span class="n">previous_nonzero_indexes</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a>                <span class="c1"># We create the new batch norm layer with the new neuron number.</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a>
</span><span id="L-428"><a href="#L-428"><span class="linenos">428</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos">429</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_neuron_number</span>
</span><span id="L-430"><a href="#L-430"><span class="linenos">430</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos">431</span></a>                <span class="n">new_bn_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">new_neuron_number</span><span class="p">,</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos">432</span></a>                                               <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos">433</span></a>                                               <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">affine</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">track_running_stats</span><span class="p">)</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos">434</span></a>
</span><span id="L-435"><a href="#L-435"><span class="linenos">435</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons from the old batch norm layer</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos">436</span></a>                <span class="c1"># to the new one. They are identified by the indexes in current_nonzero_indexes.</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos">437</span></a>
</span><span id="L-438"><a href="#L-438"><span class="linenos">438</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos">439</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos">440</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">running_mean</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos">441</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">running_var</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos">442</span></a>
</span><span id="L-443"><a href="#L-443"><span class="linenos">443</span></a>                <span class="c1"># If the old linear layer had bias then also the new linear layer has them.</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos">444</span></a>                <span class="k">if</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos">445</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-446"><a href="#L-446"><span class="linenos">446</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos">447</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos">448</span></a>
</span><span id="L-449"><a href="#L-449"><span class="linenos">449</span></a>                <span class="c1"># The number of input features for the new linear layer is equal to the number of non-zero elements in</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos">450</span></a>                <span class="c1"># the mask of the previous layer.</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos">451</span></a>                <span class="n">num_in_features</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span><span id="L-452"><a href="#L-452"><span class="linenos">452</span></a>
</span><span id="L-453"><a href="#L-453"><span class="linenos">453</span></a>                <span class="c1"># We create the new linear layer with the correct architecture.</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos">454</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="L-455"><a href="#L-455"><span class="linenos">455</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_in_features</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos">456</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="L-457"><a href="#L-457"><span class="linenos">457</span></a>                <span class="n">new_out_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos">458</span></a>                <span class="n">new_out_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_neuron_number</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos">459</span></a>                <span class="n">new_out_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_out_dim</span><span class="p">)</span>
</span><span id="L-460"><a href="#L-460"><span class="linenos">460</span></a>                <span class="n">new_linear_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">new_out_dim</span><span class="p">,</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos">461</span></a>                                              <span class="n">num_in_features</span><span class="p">,</span> <span class="n">new_neuron_number</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">)</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos">462</span></a>
</span><span id="L-463"><a href="#L-463"><span class="linenos">463</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons.</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos">464</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-465"><a href="#L-465"><span class="linenos">465</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">previous_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos">466</span></a>
</span><span id="L-467"><a href="#L-467"><span class="linenos">467</span></a>                <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos">468</span></a>                    <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos">469</span></a>
</span><span id="L-470"><a href="#L-470"><span class="linenos">470</span></a>                <span class="c1"># We save the new layers in the order in which they should be in our sequential model: first the linear</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos">471</span></a>                <span class="c1"># layer and then the batch norm layer.</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos">472</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_linear_layer</span><span class="p">)</span>
</span><span id="L-473"><a href="#L-473"><span class="linenos">473</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_bn_layer</span><span class="p">)</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos">474</span></a>
</span><span id="L-475"><a href="#L-475"><span class="linenos">475</span></a>                <span class="c1"># We update the value of previous_layer_mask with the current mask.</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos">476</span></a>                <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="n">layer_mask</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos">477</span></a>
</span><span id="L-478"><a href="#L-478"><span class="linenos">478</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="L-479"><a href="#L-479"><span class="linenos">479</span></a>
</span><span id="L-480"><a href="#L-480"><span class="linenos">480</span></a>                <span class="c1"># If the layer old_layers[i] is the first linear layer then the previous layer mask correspond to the</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos">481</span></a>                <span class="c1"># complete input.</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos">482</span></a>                <span class="k">if</span> <span class="n">previous_layer_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-483"><a href="#L-483"><span class="linenos">483</span></a>                    <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
</span><span id="L-484"><a href="#L-484"><span class="linenos">484</span></a>
</span><span id="L-485"><a href="#L-485"><span class="linenos">485</span></a>                <span class="c1"># If the linear layer is not followed by a batch normalization layer then it will not be neuron pruned,</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos">486</span></a>                <span class="c1"># therefore the layer_mask will be equals to the number of output features of the old layer.</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos">487</span></a>                <span class="n">layer_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos">488</span></a>
</span><span id="L-489"><a href="#L-489"><span class="linenos">489</span></a>                <span class="c1"># We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos">490</span></a>                <span class="n">current_nonzero_indexes</span> <span class="o">=</span> <span class="n">layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos">491</span></a>                <span class="n">previous_nonzero_indexes</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos">492</span></a>
</span><span id="L-493"><a href="#L-493"><span class="linenos">493</span></a>                <span class="k">if</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos">494</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos">495</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos">496</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-497"><a href="#L-497"><span class="linenos">497</span></a>
</span><span id="L-498"><a href="#L-498"><span class="linenos">498</span></a>                <span class="c1"># The number of input features for the new linear layer is equal to the number of non-zero elements in</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos">499</span></a>                <span class="c1"># the mask of the previous layer.</span>
</span><span id="L-500"><a href="#L-500"><span class="linenos">500</span></a>                <span class="n">num_in_features</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos">501</span></a>
</span><span id="L-502"><a href="#L-502"><span class="linenos">502</span></a>                <span class="c1"># We create the new linear layer with the correct architecture.</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos">503</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos">504</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_in_features</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos">505</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos">506</span></a>                <span class="n">new_linear_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos">507</span></a>                                              <span class="n">num_in_features</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">)</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos">508</span></a>
</span><span id="L-509"><a href="#L-509"><span class="linenos">509</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons.</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos">510</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-511"><a href="#L-511"><span class="linenos">511</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">previous_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos">512</span></a>
</span><span id="L-513"><a href="#L-513"><span class="linenos">513</span></a>                <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
</span><span id="L-514"><a href="#L-514"><span class="linenos">514</span></a>                    <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span>
</span><span id="L-515"><a href="#L-515"><span class="linenos">515</span></a>
</span><span id="L-516"><a href="#L-516"><span class="linenos">516</span></a>                <span class="c1"># We save the new layers in the order in which they should be in our sequential model: first the linear</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos">517</span></a>                <span class="c1"># layer and then the batch norm layer.</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos">518</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_linear_layer</span><span class="p">)</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos">519</span></a>
</span><span id="L-520"><a href="#L-520"><span class="linenos">520</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">ReLU</span><span class="p">):</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos">521</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ptl</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">))</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos">522</span></a>
</span><span id="L-523"><a href="#L-523"><span class="linenos">523</span></a>        <span class="n">pruned_network</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">orig_seq</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">orig_seq</span><span class="o">.</span><span class="n">input_id</span><span class="p">,</span> <span class="n">new_layers</span><span class="p">)</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos">524</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span> <span class="o">=</span> <span class="n">pruned_network</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos">525</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">identifier</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">identifier</span> <span class="o">+</span> <span class="s1">&#39;_pruned&#39;</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos">526</span></a>        <span class="k">return</span> <span class="n">net</span>
</span></pre></div>


            </section>
                <section id="PruningStrategy">
                            <input id="PruningStrategy-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">PruningStrategy</span><wbr>(<span class="base">abc.ABC</span>):

                <label class="view-source-button" for="PruningStrategy-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PruningStrategy"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PruningStrategy-15"><a href="#PruningStrategy-15"><span class="linenos">15</span></a><span class="k">class</span> <span class="nc">PruningStrategy</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
</span><span id="PruningStrategy-16"><a href="#PruningStrategy-16"><span class="linenos">16</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PruningStrategy-17"><a href="#PruningStrategy-17"><span class="linenos">17</span></a><span class="sd">    An abstract class used to represent a Pruning Strategy.</span>
</span><span id="PruningStrategy-18"><a href="#PruningStrategy-18"><span class="linenos">18</span></a>
</span><span id="PruningStrategy-19"><a href="#PruningStrategy-19"><span class="linenos">19</span></a><span class="sd">    Methods</span>
</span><span id="PruningStrategy-20"><a href="#PruningStrategy-20"><span class="linenos">20</span></a><span class="sd">    ----------</span>
</span><span id="PruningStrategy-21"><a href="#PruningStrategy-21"><span class="linenos">21</span></a><span class="sd">    prune(NeuralNetwork, Dataset)</span>
</span><span id="PruningStrategy-22"><a href="#PruningStrategy-22"><span class="linenos">22</span></a><span class="sd">        Prune the neural network of interest using a pruning strategy determined in the concrete children.</span>
</span><span id="PruningStrategy-23"><a href="#PruningStrategy-23"><span class="linenos">23</span></a>
</span><span id="PruningStrategy-24"><a href="#PruningStrategy-24"><span class="linenos">24</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="PruningStrategy-25"><a href="#PruningStrategy-25"><span class="linenos">25</span></a>
</span><span id="PruningStrategy-26"><a href="#PruningStrategy-26"><span class="linenos">26</span></a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
</span><span id="PruningStrategy-27"><a href="#PruningStrategy-27"><span class="linenos">27</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="PruningStrategy-28"><a href="#PruningStrategy-28"><span class="linenos">28</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PruningStrategy-29"><a href="#PruningStrategy-29"><span class="linenos">29</span></a><span class="sd">        Prune the neural network of interest using a pruning strategy determined in the concrete children.</span>
</span><span id="PruningStrategy-30"><a href="#PruningStrategy-30"><span class="linenos">30</span></a>
</span><span id="PruningStrategy-31"><a href="#PruningStrategy-31"><span class="linenos">31</span></a><span class="sd">        Parameters</span>
</span><span id="PruningStrategy-32"><a href="#PruningStrategy-32"><span class="linenos">32</span></a><span class="sd">        ----------</span>
</span><span id="PruningStrategy-33"><a href="#PruningStrategy-33"><span class="linenos">33</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="PruningStrategy-34"><a href="#PruningStrategy-34"><span class="linenos">34</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="PruningStrategy-35"><a href="#PruningStrategy-35"><span class="linenos">35</span></a><span class="sd">        dataset: Dataset</span>
</span><span id="PruningStrategy-36"><a href="#PruningStrategy-36"><span class="linenos">36</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="PruningStrategy-37"><a href="#PruningStrategy-37"><span class="linenos">37</span></a>
</span><span id="PruningStrategy-38"><a href="#PruningStrategy-38"><span class="linenos">38</span></a><span class="sd">        Returns</span>
</span><span id="PruningStrategy-39"><a href="#PruningStrategy-39"><span class="linenos">39</span></a><span class="sd">        ----------</span>
</span><span id="PruningStrategy-40"><a href="#PruningStrategy-40"><span class="linenos">40</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="PruningStrategy-41"><a href="#PruningStrategy-41"><span class="linenos">41</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="PruningStrategy-42"><a href="#PruningStrategy-42"><span class="linenos">42</span></a>
</span><span id="PruningStrategy-43"><a href="#PruningStrategy-43"><span class="linenos">43</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PruningStrategy-44"><a href="#PruningStrategy-44"><span class="linenos">44</span></a>        <span class="k">pass</span>
</span></pre></div>


            <div class="docstring"><p>An abstract class used to represent a Pruning Strategy.</p>

<h2 id="methods">Methods</h2>

<p>prune(NeuralNetwork, Dataset)
    Prune the neural network of interest using a pruning strategy determined in the concrete children.</p>
</div>


                            <div id="PruningStrategy.prune" class="classattr">
                                        <input id="PruningStrategy.prune-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator">@abc.abstractmethod</div>

        <span class="def">def</span>
        <span class="name">prune</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">network</span><span class="p">:</span> <span class="n"><a href="../networks.html#NeuralNetwork">pynever.networks.NeuralNetwork</a></span>,</span><span class="param">	<span class="n">dataset</span><span class="p">:</span> <span class="n"><a href="../datasets.html#Dataset">pynever.datasets.Dataset</a></span></span><span class="return-annotation">) -> <span class="n"><a href="../networks.html#NeuralNetwork">pynever.networks.NeuralNetwork</a></span>:</span></span>

                <label class="view-source-button" for="PruningStrategy.prune-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PruningStrategy.prune"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PruningStrategy.prune-26"><a href="#PruningStrategy.prune-26"><span class="linenos">26</span></a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
</span><span id="PruningStrategy.prune-27"><a href="#PruningStrategy.prune-27"><span class="linenos">27</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="PruningStrategy.prune-28"><a href="#PruningStrategy.prune-28"><span class="linenos">28</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PruningStrategy.prune-29"><a href="#PruningStrategy.prune-29"><span class="linenos">29</span></a><span class="sd">        Prune the neural network of interest using a pruning strategy determined in the concrete children.</span>
</span><span id="PruningStrategy.prune-30"><a href="#PruningStrategy.prune-30"><span class="linenos">30</span></a>
</span><span id="PruningStrategy.prune-31"><a href="#PruningStrategy.prune-31"><span class="linenos">31</span></a><span class="sd">        Parameters</span>
</span><span id="PruningStrategy.prune-32"><a href="#PruningStrategy.prune-32"><span class="linenos">32</span></a><span class="sd">        ----------</span>
</span><span id="PruningStrategy.prune-33"><a href="#PruningStrategy.prune-33"><span class="linenos">33</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="PruningStrategy.prune-34"><a href="#PruningStrategy.prune-34"><span class="linenos">34</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="PruningStrategy.prune-35"><a href="#PruningStrategy.prune-35"><span class="linenos">35</span></a><span class="sd">        dataset: Dataset</span>
</span><span id="PruningStrategy.prune-36"><a href="#PruningStrategy.prune-36"><span class="linenos">36</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="PruningStrategy.prune-37"><a href="#PruningStrategy.prune-37"><span class="linenos">37</span></a>
</span><span id="PruningStrategy.prune-38"><a href="#PruningStrategy.prune-38"><span class="linenos">38</span></a><span class="sd">        Returns</span>
</span><span id="PruningStrategy.prune-39"><a href="#PruningStrategy.prune-39"><span class="linenos">39</span></a><span class="sd">        ----------</span>
</span><span id="PruningStrategy.prune-40"><a href="#PruningStrategy.prune-40"><span class="linenos">40</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="PruningStrategy.prune-41"><a href="#PruningStrategy.prune-41"><span class="linenos">41</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="PruningStrategy.prune-42"><a href="#PruningStrategy.prune-42"><span class="linenos">42</span></a>
</span><span id="PruningStrategy.prune-43"><a href="#PruningStrategy.prune-43"><span class="linenos">43</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PruningStrategy.prune-44"><a href="#PruningStrategy.prune-44"><span class="linenos">44</span></a>        <span class="k">pass</span>
</span></pre></div>


            <div class="docstring"><p>Prune the neural network of interest using a pruning strategy determined in the concrete children.</p>

<h2 id="parameters">Parameters</h2>

<p>network : NeuralNetwork
    The neural network to prune.
dataset: Dataset
    The dataset to use for the pre-training and fine-tuning procedure.</p>

<h2 id="returns">Returns</h2>

<p>NeuralNetwork
    The Neural Network resulting from the application of the pruning strategy to the original network.</p>
</div>


                            </div>
                </section>
                <section id="WPTransform">
                            <input id="WPTransform-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">WPTransform</span>:

                <label class="view-source-button" for="WPTransform-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WPTransform"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WPTransform-47"><a href="#WPTransform-47"><span class="linenos">47</span></a><span class="k">class</span> <span class="nc">WPTransform</span><span class="p">:</span>
</span><span id="WPTransform-48"><a href="#WPTransform-48"><span class="linenos">48</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="WPTransform-49"><a href="#WPTransform-49"><span class="linenos">49</span></a><span class="sd">    Callable to pass to the training strategy used in the pruning in order to optimize it for the pruning.</span>
</span><span id="WPTransform-50"><a href="#WPTransform-50"><span class="linenos">50</span></a>
</span><span id="WPTransform-51"><a href="#WPTransform-51"><span class="linenos">51</span></a><span class="sd">    Attributes</span>
</span><span id="WPTransform-52"><a href="#WPTransform-52"><span class="linenos">52</span></a><span class="sd">    ----------</span>
</span><span id="WPTransform-53"><a href="#WPTransform-53"><span class="linenos">53</span></a><span class="sd">    l1_decay : float</span>
</span><span id="WPTransform-54"><a href="#WPTransform-54"><span class="linenos">54</span></a><span class="sd">        Coefficient of the L1 norm regularizer used on the weights of the linear layers of the network</span>
</span><span id="WPTransform-55"><a href="#WPTransform-55"><span class="linenos">55</span></a><span class="sd">        to push the weights to near-to-zero values.</span>
</span><span id="WPTransform-56"><a href="#WPTransform-56"><span class="linenos">56</span></a><span class="sd">    fine_tuning : bool</span>
</span><span id="WPTransform-57"><a href="#WPTransform-57"><span class="linenos">57</span></a><span class="sd">        If True the weight with value zero are not updated by the optimizer step, otherwise the L1 regularizer</span>
</span><span id="WPTransform-58"><a href="#WPTransform-58"><span class="linenos">58</span></a><span class="sd">        is used.</span>
</span><span id="WPTransform-59"><a href="#WPTransform-59"><span class="linenos">59</span></a><span class="sd">    cuda : bool, Optional</span>
</span><span id="WPTransform-60"><a href="#WPTransform-60"><span class="linenos">60</span></a><span class="sd">        It should be the same of the training strategy receiving the callable as network transform</span>
</span><span id="WPTransform-61"><a href="#WPTransform-61"><span class="linenos">61</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="WPTransform-62"><a href="#WPTransform-62"><span class="linenos">62</span></a>
</span><span id="WPTransform-63"><a href="#WPTransform-63"><span class="linenos">63</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="WPTransform-64"><a href="#WPTransform-64"><span class="linenos">64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">l1_decay</span> <span class="o">=</span> <span class="n">l1_decay</span>
</span><span id="WPTransform-65"><a href="#WPTransform-65"><span class="linenos">65</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="WPTransform-66"><a href="#WPTransform-66"><span class="linenos">66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">cuda</span>
</span><span id="WPTransform-67"><a href="#WPTransform-67"><span class="linenos">67</span></a>
</span><span id="WPTransform-68"><a href="#WPTransform-68"><span class="linenos">68</span></a>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="WPTransform-69"><a href="#WPTransform-69"><span class="linenos">69</span></a>
</span><span id="WPTransform-70"><a href="#WPTransform-70"><span class="linenos">70</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="WPTransform-71"><a href="#WPTransform-71"><span class="linenos">71</span></a>
</span><span id="WPTransform-72"><a href="#WPTransform-72"><span class="linenos">72</span></a>            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="WPTransform-73"><a href="#WPTransform-73"><span class="linenos">73</span></a>
</span><span id="WPTransform-74"><a href="#WPTransform-74"><span class="linenos">74</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="WPTransform-75"><a href="#WPTransform-75"><span class="linenos">75</span></a>                    <span class="n">weight_copy</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="WPTransform-76"><a href="#WPTransform-76"><span class="linenos">76</span></a>                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
</span><span id="WPTransform-77"><a href="#WPTransform-77"><span class="linenos">77</span></a>                        <span class="n">mask</span> <span class="o">=</span> <span class="n">weight_copy</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span><span id="WPTransform-78"><a href="#WPTransform-78"><span class="linenos">78</span></a>                    <span class="k">else</span><span class="p">:</span>
</span><span id="WPTransform-79"><a href="#WPTransform-79"><span class="linenos">79</span></a>                        <span class="n">mask</span> <span class="o">=</span> <span class="n">weight_copy</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="WPTransform-80"><a href="#WPTransform-80"><span class="linenos">80</span></a>                    <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</span><span id="WPTransform-81"><a href="#WPTransform-81"><span class="linenos">81</span></a>
</span><span id="WPTransform-82"><a href="#WPTransform-82"><span class="linenos">82</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="WPTransform-83"><a href="#WPTransform-83"><span class="linenos">83</span></a>
</span><span id="WPTransform-84"><a href="#WPTransform-84"><span class="linenos">84</span></a>            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="WPTransform-85"><a href="#WPTransform-85"><span class="linenos">85</span></a>
</span><span id="WPTransform-86"><a href="#WPTransform-86"><span class="linenos">86</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="WPTransform-87"><a href="#WPTransform-87"><span class="linenos">87</span></a>                    <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_decay</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span></pre></div>


            <div class="docstring"><p>Callable to pass to the training strategy used in the pruning in order to optimize it for the pruning.</p>

<h2 id="attributes">Attributes</h2>

<p>l1_decay : float
    Coefficient of the L1 norm regularizer used on the weights of the linear layers of the network
    to push the weights to near-to-zero values.
fine_tuning : bool
    If True the weight with value zero are not updated by the optimizer step, otherwise the L1 regularizer
    is used.
cuda : bool, Optional
    It should be the same of the training strategy receiving the callable as network transform</p>
</div>


                            <div id="WPTransform.__init__" class="classattr">
                                        <input id="WPTransform.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">WPTransform</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">l1_decay</span><span class="p">:</span> <span class="nb">float</span>, </span><span class="param"><span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span>, </span><span class="param"><span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="WPTransform.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WPTransform.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WPTransform.__init__-63"><a href="#WPTransform.__init__-63"><span class="linenos">63</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="WPTransform.__init__-64"><a href="#WPTransform.__init__-64"><span class="linenos">64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">l1_decay</span> <span class="o">=</span> <span class="n">l1_decay</span>
</span><span id="WPTransform.__init__-65"><a href="#WPTransform.__init__-65"><span class="linenos">65</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="WPTransform.__init__-66"><a href="#WPTransform.__init__-66"><span class="linenos">66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">cuda</span>
</span></pre></div>


    

                            </div>
                            <div id="WPTransform.l1_decay" class="classattr">
                                <div class="attr variable">
            <span class="name">l1_decay</span>

        
    </div>
    <a class="headerlink" href="#WPTransform.l1_decay"></a>
    
    

                            </div>
                            <div id="WPTransform.fine_tuning" class="classattr">
                                <div class="attr variable">
            <span class="name">fine_tuning</span>

        
    </div>
    <a class="headerlink" href="#WPTransform.fine_tuning"></a>
    
    

                            </div>
                            <div id="WPTransform.cuda" class="classattr">
                                <div class="attr variable">
            <span class="name">cuda</span>

        
    </div>
    <a class="headerlink" href="#WPTransform.cuda"></a>
    
    

                            </div>
                </section>
                <section id="NSTransform">
                            <input id="NSTransform-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">NSTransform</span>:

                <label class="view-source-button" for="NSTransform-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NSTransform"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NSTransform-90"><a href="#NSTransform-90"><span class="linenos"> 90</span></a><span class="k">class</span> <span class="nc">NSTransform</span><span class="p">:</span>
</span><span id="NSTransform-91"><a href="#NSTransform-91"><span class="linenos"> 91</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="NSTransform-92"><a href="#NSTransform-92"><span class="linenos"> 92</span></a><span class="sd">    Callable to pass to the training strategy used in the pruning in order to optimize it for the pruning.</span>
</span><span id="NSTransform-93"><a href="#NSTransform-93"><span class="linenos"> 93</span></a>
</span><span id="NSTransform-94"><a href="#NSTransform-94"><span class="linenos"> 94</span></a><span class="sd">    Attributes</span>
</span><span id="NSTransform-95"><a href="#NSTransform-95"><span class="linenos"> 95</span></a><span class="sd">    ----------</span>
</span><span id="NSTransform-96"><a href="#NSTransform-96"><span class="linenos"> 96</span></a><span class="sd">    batchnorm_decay : float</span>
</span><span id="NSTransform-97"><a href="#NSTransform-97"><span class="linenos"> 97</span></a><span class="sd">        Coefficient of the L1 norm regularizer used on the batchnorm layers of the network</span>
</span><span id="NSTransform-98"><a href="#NSTransform-98"><span class="linenos"> 98</span></a><span class="sd">        to push the weights to near-to-zero values.</span>
</span><span id="NSTransform-99"><a href="#NSTransform-99"><span class="linenos"> 99</span></a><span class="sd">    fine_tuning : bool</span>
</span><span id="NSTransform-100"><a href="#NSTransform-100"><span class="linenos">100</span></a><span class="sd">        If True the L1 regularizer is not applied to the network.</span>
</span><span id="NSTransform-101"><a href="#NSTransform-101"><span class="linenos">101</span></a><span class="sd">    cuda : bool, Optional</span>
</span><span id="NSTransform-102"><a href="#NSTransform-102"><span class="linenos">102</span></a><span class="sd">        It should be the same of the training strategy receiving the callable as network transform</span>
</span><span id="NSTransform-103"><a href="#NSTransform-103"><span class="linenos">103</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="NSTransform-104"><a href="#NSTransform-104"><span class="linenos">104</span></a>
</span><span id="NSTransform-105"><a href="#NSTransform-105"><span class="linenos">105</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchnorm_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="NSTransform-106"><a href="#NSTransform-106"><span class="linenos">106</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm_decay</span> <span class="o">=</span> <span class="n">batchnorm_decay</span>
</span><span id="NSTransform-107"><a href="#NSTransform-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="NSTransform-108"><a href="#NSTransform-108"><span class="linenos">108</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">cuda</span>
</span><span id="NSTransform-109"><a href="#NSTransform-109"><span class="linenos">109</span></a>
</span><span id="NSTransform-110"><a href="#NSTransform-110"><span class="linenos">110</span></a>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="NSTransform-111"><a href="#NSTransform-111"><span class="linenos">111</span></a>
</span><span id="NSTransform-112"><a href="#NSTransform-112"><span class="linenos">112</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="NSTransform-113"><a href="#NSTransform-113"><span class="linenos">113</span></a>            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="NSTransform-114"><a href="#NSTransform-114"><span class="linenos">114</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="NSTransform-115"><a href="#NSTransform-115"><span class="linenos">115</span></a>                    <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchnorm_decay</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span></pre></div>


            <div class="docstring"><p>Callable to pass to the training strategy used in the pruning in order to optimize it for the pruning.</p>

<h2 id="attributes">Attributes</h2>

<p>batchnorm_decay : float
    Coefficient of the L1 norm regularizer used on the batchnorm layers of the network
    to push the weights to near-to-zero values.
fine_tuning : bool
    If True the L1 regularizer is not applied to the network.
cuda : bool, Optional
    It should be the same of the training strategy receiving the callable as network transform</p>
</div>


                            <div id="NSTransform.__init__" class="classattr">
                                        <input id="NSTransform.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">NSTransform</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">batchnorm_decay</span><span class="p">:</span> <span class="nb">float</span>, </span><span class="param"><span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span>, </span><span class="param"><span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="NSTransform.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NSTransform.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NSTransform.__init__-105"><a href="#NSTransform.__init__-105"><span class="linenos">105</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchnorm_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fine_tuning</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">cuda</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="NSTransform.__init__-106"><a href="#NSTransform.__init__-106"><span class="linenos">106</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchnorm_decay</span> <span class="o">=</span> <span class="n">batchnorm_decay</span>
</span><span id="NSTransform.__init__-107"><a href="#NSTransform.__init__-107"><span class="linenos">107</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="NSTransform.__init__-108"><a href="#NSTransform.__init__-108"><span class="linenos">108</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">cuda</span>
</span></pre></div>


    

                            </div>
                            <div id="NSTransform.batchnorm_decay" class="classattr">
                                <div class="attr variable">
            <span class="name">batchnorm_decay</span>

        
    </div>
    <a class="headerlink" href="#NSTransform.batchnorm_decay"></a>
    
    

                            </div>
                            <div id="NSTransform.fine_tuning" class="classattr">
                                <div class="attr variable">
            <span class="name">fine_tuning</span>

        
    </div>
    <a class="headerlink" href="#NSTransform.fine_tuning"></a>
    
    

                            </div>
                            <div id="NSTransform.cuda" class="classattr">
                                <div class="attr variable">
            <span class="name">cuda</span>

        
    </div>
    <a class="headerlink" href="#NSTransform.cuda"></a>
    
    

                            </div>
                </section>
                <section id="WeightPruning">
                            <input id="WeightPruning-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">WeightPruning</span><wbr>(<span class="base"><a href="#PruningStrategy">PruningStrategy</a></span>):

                <label class="view-source-button" for="WeightPruning-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WeightPruning"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WeightPruning-118"><a href="#WeightPruning-118"><span class="linenos">118</span></a><span class="k">class</span> <span class="nc">WeightPruning</span><span class="p">(</span><span class="n">PruningStrategy</span><span class="p">):</span>
</span><span id="WeightPruning-119"><a href="#WeightPruning-119"><span class="linenos">119</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="WeightPruning-120"><a href="#WeightPruning-120"><span class="linenos">120</span></a><span class="sd">    A concrete class used to represent the weight pruning strategy.</span>
</span><span id="WeightPruning-121"><a href="#WeightPruning-121"><span class="linenos">121</span></a><span class="sd">    This kind of pruning select the least important weights of the neural network</span>
</span><span id="WeightPruning-122"><a href="#WeightPruning-122"><span class="linenos">122</span></a><span class="sd">    of interest and set them to 0. It assume vectorial input for the linear layers.</span>
</span><span id="WeightPruning-123"><a href="#WeightPruning-123"><span class="linenos">123</span></a><span class="sd">    We refer to https://arxiv.org/abs/1506.02626 for theoretical details on the strategy.</span>
</span><span id="WeightPruning-124"><a href="#WeightPruning-124"><span class="linenos">124</span></a>
</span><span id="WeightPruning-125"><a href="#WeightPruning-125"><span class="linenos">125</span></a><span class="sd">    Attributes</span>
</span><span id="WeightPruning-126"><a href="#WeightPruning-126"><span class="linenos">126</span></a><span class="sd">    ----------</span>
</span><span id="WeightPruning-127"><a href="#WeightPruning-127"><span class="linenos">127</span></a><span class="sd">    sparsity_rate : float</span>
</span><span id="WeightPruning-128"><a href="#WeightPruning-128"><span class="linenos">128</span></a><span class="sd">        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.</span>
</span><span id="WeightPruning-129"><a href="#WeightPruning-129"><span class="linenos">129</span></a><span class="sd">    training_strategy : PytorchTraining</span>
</span><span id="WeightPruning-130"><a href="#WeightPruning-130"><span class="linenos">130</span></a><span class="sd">        The training strategy to use for pre-training and/or fine-tuning. NB: Its network_transform parameter must</span>
</span><span id="WeightPruning-131"><a href="#WeightPruning-131"><span class="linenos">131</span></a><span class="sd">        be of the class WPTransform.</span>
</span><span id="WeightPruning-132"><a href="#WeightPruning-132"><span class="linenos">132</span></a><span class="sd">    pre_training : bool</span>
</span><span id="WeightPruning-133"><a href="#WeightPruning-133"><span class="linenos">133</span></a><span class="sd">        Flag to indicate if the network need to be pre-trained.</span>
</span><span id="WeightPruning-134"><a href="#WeightPruning-134"><span class="linenos">134</span></a>
</span><span id="WeightPruning-135"><a href="#WeightPruning-135"><span class="linenos">135</span></a><span class="sd">    Methods</span>
</span><span id="WeightPruning-136"><a href="#WeightPruning-136"><span class="linenos">136</span></a><span class="sd">    ----------</span>
</span><span id="WeightPruning-137"><a href="#WeightPruning-137"><span class="linenos">137</span></a><span class="sd">    prune(NeuralNetwork, Dataset)</span>
</span><span id="WeightPruning-138"><a href="#WeightPruning-138"><span class="linenos">138</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Weight Pruning.</span>
</span><span id="WeightPruning-139"><a href="#WeightPruning-139"><span class="linenos">139</span></a>
</span><span id="WeightPruning-140"><a href="#WeightPruning-140"><span class="linenos">140</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="WeightPruning-141"><a href="#WeightPruning-141"><span class="linenos">141</span></a>
</span><span id="WeightPruning-142"><a href="#WeightPruning-142"><span class="linenos">142</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">training_strategy</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">PytorchTraining</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="WeightPruning-143"><a href="#WeightPruning-143"><span class="linenos">143</span></a>                 <span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="WeightPruning-144"><a href="#WeightPruning-144"><span class="linenos">144</span></a>
</span><span id="WeightPruning-145"><a href="#WeightPruning-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span> <span class="o">=</span> <span class="n">sparsity_rate</span>
</span><span id="WeightPruning-146"><a href="#WeightPruning-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="o">=</span> <span class="n">training_strategy</span>
</span><span id="WeightPruning-147"><a href="#WeightPruning-147"><span class="linenos">147</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="p">,</span> <span class="n">WPTransform</span><span class="p">):</span>
</span><span id="WeightPruning-148"><a href="#WeightPruning-148"><span class="linenos">148</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The network_transform attribute of the training_strategy should be of&quot;</span>
</span><span id="WeightPruning-149"><a href="#WeightPruning-149"><span class="linenos">149</span></a>                            <span class="s2">&quot; the class WPTransform&quot;</span><span class="p">)</span>
</span><span id="WeightPruning-150"><a href="#WeightPruning-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span> <span class="o">=</span> <span class="n">pre_training</span>
</span><span id="WeightPruning-151"><a href="#WeightPruning-151"><span class="linenos">151</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pre_training</span><span class="p">:</span>
</span><span id="WeightPruning-152"><a href="#WeightPruning-152"><span class="linenos">152</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;If pre_training is True then training_strategy must not be None&quot;</span><span class="p">)</span>
</span><span id="WeightPruning-153"><a href="#WeightPruning-153"><span class="linenos">153</span></a>
</span><span id="WeightPruning-154"><a href="#WeightPruning-154"><span class="linenos">154</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="WeightPruning-155"><a href="#WeightPruning-155"><span class="linenos">155</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="WeightPruning-156"><a href="#WeightPruning-156"><span class="linenos">156</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Weight Pruning.</span>
</span><span id="WeightPruning-157"><a href="#WeightPruning-157"><span class="linenos">157</span></a>
</span><span id="WeightPruning-158"><a href="#WeightPruning-158"><span class="linenos">158</span></a><span class="sd">        Parameters</span>
</span><span id="WeightPruning-159"><a href="#WeightPruning-159"><span class="linenos">159</span></a><span class="sd">        ----------</span>
</span><span id="WeightPruning-160"><a href="#WeightPruning-160"><span class="linenos">160</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="WeightPruning-161"><a href="#WeightPruning-161"><span class="linenos">161</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="WeightPruning-162"><a href="#WeightPruning-162"><span class="linenos">162</span></a><span class="sd">        dataset : Dataset</span>
</span><span id="WeightPruning-163"><a href="#WeightPruning-163"><span class="linenos">163</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="WeightPruning-164"><a href="#WeightPruning-164"><span class="linenos">164</span></a>
</span><span id="WeightPruning-165"><a href="#WeightPruning-165"><span class="linenos">165</span></a><span class="sd">        Returns</span>
</span><span id="WeightPruning-166"><a href="#WeightPruning-166"><span class="linenos">166</span></a><span class="sd">        ----------</span>
</span><span id="WeightPruning-167"><a href="#WeightPruning-167"><span class="linenos">167</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="WeightPruning-168"><a href="#WeightPruning-168"><span class="linenos">168</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="WeightPruning-169"><a href="#WeightPruning-169"><span class="linenos">169</span></a>
</span><span id="WeightPruning-170"><a href="#WeightPruning-170"><span class="linenos">170</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WeightPruning-171"><a href="#WeightPruning-171"><span class="linenos">171</span></a>
</span><span id="WeightPruning-172"><a href="#WeightPruning-172"><span class="linenos">172</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span><span class="p">:</span>
</span><span id="WeightPruning-173"><a href="#WeightPruning-173"><span class="linenos">173</span></a>            <span class="n">fine_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span>
</span><span id="WeightPruning-174"><a href="#WeightPruning-174"><span class="linenos">174</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="WeightPruning-175"><a href="#WeightPruning-175"><span class="linenos">175</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="WeightPruning-176"><a href="#WeightPruning-176"><span class="linenos">176</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="WeightPruning-177"><a href="#WeightPruning-177"><span class="linenos">177</span></a>
</span><span id="WeightPruning-178"><a href="#WeightPruning-178"><span class="linenos">178</span></a>        <span class="n">pytorch_converter</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchConverter</span><span class="p">()</span>
</span><span id="WeightPruning-179"><a href="#WeightPruning-179"><span class="linenos">179</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="n">pytorch_converter</span><span class="o">.</span><span class="n">from_neural_network</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</span><span id="WeightPruning-180"><a href="#WeightPruning-180"><span class="linenos">180</span></a>
</span><span id="WeightPruning-181"><a href="#WeightPruning-181"><span class="linenos">181</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pruning</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="WeightPruning-182"><a href="#WeightPruning-182"><span class="linenos">182</span></a>
</span><span id="WeightPruning-183"><a href="#WeightPruning-183"><span class="linenos">183</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</span><span id="WeightPruning-184"><a href="#WeightPruning-184"><span class="linenos">184</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="WeightPruning-185"><a href="#WeightPruning-185"><span class="linenos">185</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">up_to_date</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="WeightPruning-186"><a href="#WeightPruning-186"><span class="linenos">186</span></a>
</span><span id="WeightPruning-187"><a href="#WeightPruning-187"><span class="linenos">187</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="WeightPruning-188"><a href="#WeightPruning-188"><span class="linenos">188</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="WeightPruning-189"><a href="#WeightPruning-189"><span class="linenos">189</span></a>
</span><span id="WeightPruning-190"><a href="#WeightPruning-190"><span class="linenos">190</span></a>        <span class="k">return</span> <span class="n">network</span>
</span><span id="WeightPruning-191"><a href="#WeightPruning-191"><span class="linenos">191</span></a>
</span><span id="WeightPruning-192"><a href="#WeightPruning-192"><span class="linenos">192</span></a>    <span class="k">def</span> <span class="nf">__pruning</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="WeightPruning-193"><a href="#WeightPruning-193"><span class="linenos">193</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="WeightPruning-194"><a href="#WeightPruning-194"><span class="linenos">194</span></a><span class="sd">        Procedure for the pruning of the weights of the PyTorchNetwork passed as an argument.</span>
</span><span id="WeightPruning-195"><a href="#WeightPruning-195"><span class="linenos">195</span></a>
</span><span id="WeightPruning-196"><a href="#WeightPruning-196"><span class="linenos">196</span></a><span class="sd">        Parameters</span>
</span><span id="WeightPruning-197"><a href="#WeightPruning-197"><span class="linenos">197</span></a><span class="sd">        ----------</span>
</span><span id="WeightPruning-198"><a href="#WeightPruning-198"><span class="linenos">198</span></a><span class="sd">        net : PyTorchNetwork</span>
</span><span id="WeightPruning-199"><a href="#WeightPruning-199"><span class="linenos">199</span></a><span class="sd">            The PyTorchNetwork to prune.</span>
</span><span id="WeightPruning-200"><a href="#WeightPruning-200"><span class="linenos">200</span></a>
</span><span id="WeightPruning-201"><a href="#WeightPruning-201"><span class="linenos">201</span></a><span class="sd">        Returns</span>
</span><span id="WeightPruning-202"><a href="#WeightPruning-202"><span class="linenos">202</span></a><span class="sd">        ----------</span>
</span><span id="WeightPruning-203"><a href="#WeightPruning-203"><span class="linenos">203</span></a><span class="sd">        PyTorchNetwork</span>
</span><span id="WeightPruning-204"><a href="#WeightPruning-204"><span class="linenos">204</span></a><span class="sd">            The pruned PyTorchNetwork.</span>
</span><span id="WeightPruning-205"><a href="#WeightPruning-205"><span class="linenos">205</span></a>
</span><span id="WeightPruning-206"><a href="#WeightPruning-206"><span class="linenos">206</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WeightPruning-207"><a href="#WeightPruning-207"><span class="linenos">207</span></a>
</span><span id="WeightPruning-208"><a href="#WeightPruning-208"><span class="linenos">208</span></a>        <span class="c1"># We transfer the internal pytorch model to the CPU for the pruning procedure.</span>
</span><span id="WeightPruning-209"><a href="#WeightPruning-209"><span class="linenos">209</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span><span id="WeightPruning-210"><a href="#WeightPruning-210"><span class="linenos">210</span></a>
</span><span id="WeightPruning-211"><a href="#WeightPruning-211"><span class="linenos">211</span></a>        <span class="c1"># We compute the number of weights in the network</span>
</span><span id="WeightPruning-212"><a href="#WeightPruning-212"><span class="linenos">212</span></a>        <span class="n">num_weights</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="WeightPruning-213"><a href="#WeightPruning-213"><span class="linenos">213</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="WeightPruning-214"><a href="#WeightPruning-214"><span class="linenos">214</span></a>
</span><span id="WeightPruning-215"><a href="#WeightPruning-215"><span class="linenos">215</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="WeightPruning-216"><a href="#WeightPruning-216"><span class="linenos">216</span></a>                <span class="n">num_weights</span> <span class="o">+=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="WeightPruning-217"><a href="#WeightPruning-217"><span class="linenos">217</span></a>
</span><span id="WeightPruning-218"><a href="#WeightPruning-218"><span class="linenos">218</span></a>        <span class="c1"># We copy all the absolute values of the weights in a new tensor and we sort in ascending order</span>
</span><span id="WeightPruning-219"><a href="#WeightPruning-219"><span class="linenos">219</span></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_weights</span><span class="p">)</span>
</span><span id="WeightPruning-220"><a href="#WeightPruning-220"><span class="linenos">220</span></a>        <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="WeightPruning-221"><a href="#WeightPruning-221"><span class="linenos">221</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="WeightPruning-222"><a href="#WeightPruning-222"><span class="linenos">222</span></a>
</span><span id="WeightPruning-223"><a href="#WeightPruning-223"><span class="linenos">223</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="WeightPruning-224"><a href="#WeightPruning-224"><span class="linenos">224</span></a>                <span class="n">size</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="WeightPruning-225"><a href="#WeightPruning-225"><span class="linenos">225</span></a>                <span class="n">weights</span><span class="p">[</span><span class="n">index</span><span class="p">:(</span><span class="n">index</span> <span class="o">+</span> <span class="n">size</span><span class="p">)]</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="WeightPruning-226"><a href="#WeightPruning-226"><span class="linenos">226</span></a>                <span class="n">index</span> <span class="o">+=</span> <span class="n">size</span>
</span><span id="WeightPruning-227"><a href="#WeightPruning-227"><span class="linenos">227</span></a>
</span><span id="WeightPruning-228"><a href="#WeightPruning-228"><span class="linenos">228</span></a>        <span class="n">ordered_weights</span><span class="p">,</span> <span class="n">ordered_indexes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</span><span id="WeightPruning-229"><a href="#WeightPruning-229"><span class="linenos">229</span></a>
</span><span id="WeightPruning-230"><a href="#WeightPruning-230"><span class="linenos">230</span></a>        <span class="c1"># We determine the number of weights we need to set to 0, given the sparsity rate.</span>
</span><span id="WeightPruning-231"><a href="#WeightPruning-231"><span class="linenos">231</span></a>        <span class="n">threshold_index</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">num_weights</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span><span class="p">)</span>
</span><span id="WeightPruning-232"><a href="#WeightPruning-232"><span class="linenos">232</span></a>
</span><span id="WeightPruning-233"><a href="#WeightPruning-233"><span class="linenos">233</span></a>        <span class="c1"># We select the weight absolute value we will use as threshold value given the threshold index.</span>
</span><span id="WeightPruning-234"><a href="#WeightPruning-234"><span class="linenos">234</span></a>        <span class="n">threshold_value</span> <span class="o">=</span> <span class="n">ordered_weights</span><span class="p">[</span><span class="n">threshold_index</span><span class="p">]</span>
</span><span id="WeightPruning-235"><a href="#WeightPruning-235"><span class="linenos">235</span></a>
</span><span id="WeightPruning-236"><a href="#WeightPruning-236"><span class="linenos">236</span></a>        <span class="c1"># We set all the weights of the different layers to 0 if they are less or equal than the threshold value</span>
</span><span id="WeightPruning-237"><a href="#WeightPruning-237"><span class="linenos">237</span></a>        <span class="c1"># (in absolute value)</span>
</span><span id="WeightPruning-238"><a href="#WeightPruning-238"><span class="linenos">238</span></a>
</span><span id="WeightPruning-239"><a href="#WeightPruning-239"><span class="linenos">239</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="WeightPruning-240"><a href="#WeightPruning-240"><span class="linenos">240</span></a>
</span><span id="WeightPruning-241"><a href="#WeightPruning-241"><span class="linenos">241</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="WeightPruning-242"><a href="#WeightPruning-242"><span class="linenos">242</span></a>                <span class="c1"># The values of the mask are 0 when the corresponding weight is less then the threshold_value (in</span>
</span><span id="WeightPruning-243"><a href="#WeightPruning-243"><span class="linenos">243</span></a>                <span class="c1"># absolute value), otherwise they are 1</span>
</span><span id="WeightPruning-244"><a href="#WeightPruning-244"><span class="linenos">244</span></a>                <span class="n">mask</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">threshold_value</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="WeightPruning-245"><a href="#WeightPruning-245"><span class="linenos">245</span></a>                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span><span id="WeightPruning-246"><a href="#WeightPruning-246"><span class="linenos">246</span></a>
</span><span id="WeightPruning-247"><a href="#WeightPruning-247"><span class="linenos">247</span></a>        <span class="k">return</span> <span class="n">net</span>
</span></pre></div>


            <div class="docstring"><p>A concrete class used to represent the weight pruning strategy.
This kind of pruning select the least important weights of the neural network
of interest and set them to 0. It assume vectorial input for the linear layers.
We refer to <a href="https://arxiv.org/abs/1506.02626">https://arxiv.org/abs/1506.02626</a> for theoretical details on the strategy.</p>

<h2 id="attributes">Attributes</h2>

<p>sparsity_rate : float
    It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.
training_strategy : PytorchTraining
    The training strategy to use for pre-training and/or fine-tuning. NB: Its network_transform parameter must
    be of the class WPTransform.
pre_training : bool
    Flag to indicate if the network need to be pre-trained.</p>

<h2 id="methods">Methods</h2>

<p>prune(NeuralNetwork, Dataset)
    Prune the neural network of interest using the pruning strategy Weight Pruning.</p>
</div>


                            <div id="WeightPruning.__init__" class="classattr">
                                        <input id="WeightPruning.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">WeightPruning</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">training_strategy</span><span class="p">:</span> <span class="n"><a href="training.html#PytorchTraining">pynever.strategies.training.PytorchTraining</a></span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="WeightPruning.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WeightPruning.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WeightPruning.__init__-142"><a href="#WeightPruning.__init__-142"><span class="linenos">142</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">training_strategy</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">PytorchTraining</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="WeightPruning.__init__-143"><a href="#WeightPruning.__init__-143"><span class="linenos">143</span></a>                 <span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="WeightPruning.__init__-144"><a href="#WeightPruning.__init__-144"><span class="linenos">144</span></a>
</span><span id="WeightPruning.__init__-145"><a href="#WeightPruning.__init__-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span> <span class="o">=</span> <span class="n">sparsity_rate</span>
</span><span id="WeightPruning.__init__-146"><a href="#WeightPruning.__init__-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="o">=</span> <span class="n">training_strategy</span>
</span><span id="WeightPruning.__init__-147"><a href="#WeightPruning.__init__-147"><span class="linenos">147</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="p">,</span> <span class="n">WPTransform</span><span class="p">):</span>
</span><span id="WeightPruning.__init__-148"><a href="#WeightPruning.__init__-148"><span class="linenos">148</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The network_transform attribute of the training_strategy should be of&quot;</span>
</span><span id="WeightPruning.__init__-149"><a href="#WeightPruning.__init__-149"><span class="linenos">149</span></a>                            <span class="s2">&quot; the class WPTransform&quot;</span><span class="p">)</span>
</span><span id="WeightPruning.__init__-150"><a href="#WeightPruning.__init__-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span> <span class="o">=</span> <span class="n">pre_training</span>
</span><span id="WeightPruning.__init__-151"><a href="#WeightPruning.__init__-151"><span class="linenos">151</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pre_training</span><span class="p">:</span>
</span><span id="WeightPruning.__init__-152"><a href="#WeightPruning.__init__-152"><span class="linenos">152</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;If pre_training is True then training_strategy must not be None&quot;</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="WeightPruning.sparsity_rate" class="classattr">
                                <div class="attr variable">
            <span class="name">sparsity_rate</span>

        
    </div>
    <a class="headerlink" href="#WeightPruning.sparsity_rate"></a>
    
    

                            </div>
                            <div id="WeightPruning.training_strategy" class="classattr">
                                <div class="attr variable">
            <span class="name">training_strategy</span>

        
    </div>
    <a class="headerlink" href="#WeightPruning.training_strategy"></a>
    
    

                            </div>
                            <div id="WeightPruning.pre_training" class="classattr">
                                <div class="attr variable">
            <span class="name">pre_training</span>

        
    </div>
    <a class="headerlink" href="#WeightPruning.pre_training"></a>
    
    

                            </div>
                            <div id="WeightPruning.prune" class="classattr">
                                        <input id="WeightPruning.prune-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">prune</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">network</span><span class="p">:</span> <span class="n"><a href="../networks.html#NeuralNetwork">pynever.networks.NeuralNetwork</a></span>,</span><span class="param">	<span class="n">dataset</span><span class="p">:</span> <span class="n"><a href="../datasets.html#Dataset">pynever.datasets.Dataset</a></span></span><span class="return-annotation">) -> <span class="n"><a href="../networks.html#NeuralNetwork">pynever.networks.NeuralNetwork</a></span>:</span></span>

                <label class="view-source-button" for="WeightPruning.prune-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#WeightPruning.prune"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="WeightPruning.prune-154"><a href="#WeightPruning.prune-154"><span class="linenos">154</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="WeightPruning.prune-155"><a href="#WeightPruning.prune-155"><span class="linenos">155</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="WeightPruning.prune-156"><a href="#WeightPruning.prune-156"><span class="linenos">156</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Weight Pruning.</span>
</span><span id="WeightPruning.prune-157"><a href="#WeightPruning.prune-157"><span class="linenos">157</span></a>
</span><span id="WeightPruning.prune-158"><a href="#WeightPruning.prune-158"><span class="linenos">158</span></a><span class="sd">        Parameters</span>
</span><span id="WeightPruning.prune-159"><a href="#WeightPruning.prune-159"><span class="linenos">159</span></a><span class="sd">        ----------</span>
</span><span id="WeightPruning.prune-160"><a href="#WeightPruning.prune-160"><span class="linenos">160</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="WeightPruning.prune-161"><a href="#WeightPruning.prune-161"><span class="linenos">161</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="WeightPruning.prune-162"><a href="#WeightPruning.prune-162"><span class="linenos">162</span></a><span class="sd">        dataset : Dataset</span>
</span><span id="WeightPruning.prune-163"><a href="#WeightPruning.prune-163"><span class="linenos">163</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="WeightPruning.prune-164"><a href="#WeightPruning.prune-164"><span class="linenos">164</span></a>
</span><span id="WeightPruning.prune-165"><a href="#WeightPruning.prune-165"><span class="linenos">165</span></a><span class="sd">        Returns</span>
</span><span id="WeightPruning.prune-166"><a href="#WeightPruning.prune-166"><span class="linenos">166</span></a><span class="sd">        ----------</span>
</span><span id="WeightPruning.prune-167"><a href="#WeightPruning.prune-167"><span class="linenos">167</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="WeightPruning.prune-168"><a href="#WeightPruning.prune-168"><span class="linenos">168</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="WeightPruning.prune-169"><a href="#WeightPruning.prune-169"><span class="linenos">169</span></a>
</span><span id="WeightPruning.prune-170"><a href="#WeightPruning.prune-170"><span class="linenos">170</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="WeightPruning.prune-171"><a href="#WeightPruning.prune-171"><span class="linenos">171</span></a>
</span><span id="WeightPruning.prune-172"><a href="#WeightPruning.prune-172"><span class="linenos">172</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span><span class="p">:</span>
</span><span id="WeightPruning.prune-173"><a href="#WeightPruning.prune-173"><span class="linenos">173</span></a>            <span class="n">fine_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span>
</span><span id="WeightPruning.prune-174"><a href="#WeightPruning.prune-174"><span class="linenos">174</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="WeightPruning.prune-175"><a href="#WeightPruning.prune-175"><span class="linenos">175</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="WeightPruning.prune-176"><a href="#WeightPruning.prune-176"><span class="linenos">176</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="WeightPruning.prune-177"><a href="#WeightPruning.prune-177"><span class="linenos">177</span></a>
</span><span id="WeightPruning.prune-178"><a href="#WeightPruning.prune-178"><span class="linenos">178</span></a>        <span class="n">pytorch_converter</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchConverter</span><span class="p">()</span>
</span><span id="WeightPruning.prune-179"><a href="#WeightPruning.prune-179"><span class="linenos">179</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="n">pytorch_converter</span><span class="o">.</span><span class="n">from_neural_network</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</span><span id="WeightPruning.prune-180"><a href="#WeightPruning.prune-180"><span class="linenos">180</span></a>
</span><span id="WeightPruning.prune-181"><a href="#WeightPruning.prune-181"><span class="linenos">181</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pruning</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="WeightPruning.prune-182"><a href="#WeightPruning.prune-182"><span class="linenos">182</span></a>
</span><span id="WeightPruning.prune-183"><a href="#WeightPruning.prune-183"><span class="linenos">183</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</span><span id="WeightPruning.prune-184"><a href="#WeightPruning.prune-184"><span class="linenos">184</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="WeightPruning.prune-185"><a href="#WeightPruning.prune-185"><span class="linenos">185</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">up_to_date</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="WeightPruning.prune-186"><a href="#WeightPruning.prune-186"><span class="linenos">186</span></a>
</span><span id="WeightPruning.prune-187"><a href="#WeightPruning.prune-187"><span class="linenos">187</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="WeightPruning.prune-188"><a href="#WeightPruning.prune-188"><span class="linenos">188</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="WeightPruning.prune-189"><a href="#WeightPruning.prune-189"><span class="linenos">189</span></a>
</span><span id="WeightPruning.prune-190"><a href="#WeightPruning.prune-190"><span class="linenos">190</span></a>        <span class="k">return</span> <span class="n">network</span>
</span></pre></div>


            <div class="docstring"><p>Prune the neural network of interest using the pruning strategy Weight Pruning.</p>

<h2 id="parameters">Parameters</h2>

<p>network : NeuralNetwork
    The neural network to prune.
dataset : Dataset
    The dataset to use for the pre-training and fine-tuning procedure.</p>

<h2 id="returns">Returns</h2>

<p>NeuralNetwork
    The Neural Network resulting from the application of the pruning strategy to the original network.</p>
</div>


                            </div>
                </section>
                <section id="NetworkSlimming">
                            <input id="NetworkSlimming-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">NetworkSlimming</span><wbr>(<span class="base"><a href="#PruningStrategy">PruningStrategy</a></span>):

                <label class="view-source-button" for="NetworkSlimming-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NetworkSlimming"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NetworkSlimming-250"><a href="#NetworkSlimming-250"><span class="linenos">250</span></a><span class="k">class</span> <span class="nc">NetworkSlimming</span><span class="p">(</span><span class="n">PruningStrategy</span><span class="p">):</span>
</span><span id="NetworkSlimming-251"><a href="#NetworkSlimming-251"><span class="linenos">251</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="NetworkSlimming-252"><a href="#NetworkSlimming-252"><span class="linenos">252</span></a><span class="sd">    A concrete class used to represent the network slimming pruning strategy.</span>
</span><span id="NetworkSlimming-253"><a href="#NetworkSlimming-253"><span class="linenos">253</span></a><span class="sd">    This kind of pruning select the least important neurons of the neural network</span>
</span><span id="NetworkSlimming-254"><a href="#NetworkSlimming-254"><span class="linenos">254</span></a><span class="sd">    of interest and eliminates them. It needs a batch normalization layer following each layer</span>
</span><span id="NetworkSlimming-255"><a href="#NetworkSlimming-255"><span class="linenos">255</span></a><span class="sd">    which should be pruned. We assume that the activation function is always applied after the batch</span>
</span><span id="NetworkSlimming-256"><a href="#NetworkSlimming-256"><span class="linenos">256</span></a><span class="sd">    normalization layer. It support only networks with linear and batchnorm layers with vectorial inputs</span>
</span><span id="NetworkSlimming-257"><a href="#NetworkSlimming-257"><span class="linenos">257</span></a><span class="sd">    We refer to https://arxiv.org/abs/1708.06519 for theoretical details on the strategy.</span>
</span><span id="NetworkSlimming-258"><a href="#NetworkSlimming-258"><span class="linenos">258</span></a>
</span><span id="NetworkSlimming-259"><a href="#NetworkSlimming-259"><span class="linenos">259</span></a><span class="sd">    Attributes</span>
</span><span id="NetworkSlimming-260"><a href="#NetworkSlimming-260"><span class="linenos">260</span></a><span class="sd">    ----------</span>
</span><span id="NetworkSlimming-261"><a href="#NetworkSlimming-261"><span class="linenos">261</span></a><span class="sd">    sparsity_rate : float</span>
</span><span id="NetworkSlimming-262"><a href="#NetworkSlimming-262"><span class="linenos">262</span></a><span class="sd">        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.</span>
</span><span id="NetworkSlimming-263"><a href="#NetworkSlimming-263"><span class="linenos">263</span></a><span class="sd">    training_strategy : PytorchTraining</span>
</span><span id="NetworkSlimming-264"><a href="#NetworkSlimming-264"><span class="linenos">264</span></a><span class="sd">        The training strategy to use for pre-training and/or fine-tuning. NB: Its network_transform parameter must</span>
</span><span id="NetworkSlimming-265"><a href="#NetworkSlimming-265"><span class="linenos">265</span></a><span class="sd">        be of the class NSTransform.</span>
</span><span id="NetworkSlimming-266"><a href="#NetworkSlimming-266"><span class="linenos">266</span></a><span class="sd">    pre_training : bool</span>
</span><span id="NetworkSlimming-267"><a href="#NetworkSlimming-267"><span class="linenos">267</span></a><span class="sd">        Flag to indicate if the network need to be pre-trained.</span>
</span><span id="NetworkSlimming-268"><a href="#NetworkSlimming-268"><span class="linenos">268</span></a>
</span><span id="NetworkSlimming-269"><a href="#NetworkSlimming-269"><span class="linenos">269</span></a><span class="sd">    Methods</span>
</span><span id="NetworkSlimming-270"><a href="#NetworkSlimming-270"><span class="linenos">270</span></a><span class="sd">    ----------</span>
</span><span id="NetworkSlimming-271"><a href="#NetworkSlimming-271"><span class="linenos">271</span></a><span class="sd">    prune(NeuralNetwork, Dataset)</span>
</span><span id="NetworkSlimming-272"><a href="#NetworkSlimming-272"><span class="linenos">272</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Network Slimming.</span>
</span><span id="NetworkSlimming-273"><a href="#NetworkSlimming-273"><span class="linenos">273</span></a>
</span><span id="NetworkSlimming-274"><a href="#NetworkSlimming-274"><span class="linenos">274</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="NetworkSlimming-275"><a href="#NetworkSlimming-275"><span class="linenos">275</span></a>
</span><span id="NetworkSlimming-276"><a href="#NetworkSlimming-276"><span class="linenos">276</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">training_strategy</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">PytorchTraining</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="NetworkSlimming-277"><a href="#NetworkSlimming-277"><span class="linenos">277</span></a>                 <span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="NetworkSlimming-278"><a href="#NetworkSlimming-278"><span class="linenos">278</span></a>
</span><span id="NetworkSlimming-279"><a href="#NetworkSlimming-279"><span class="linenos">279</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span> <span class="o">=</span> <span class="n">sparsity_rate</span>
</span><span id="NetworkSlimming-280"><a href="#NetworkSlimming-280"><span class="linenos">280</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="o">=</span> <span class="n">training_strategy</span>
</span><span id="NetworkSlimming-281"><a href="#NetworkSlimming-281"><span class="linenos">281</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="p">,</span> <span class="n">NSTransform</span><span class="p">):</span>
</span><span id="NetworkSlimming-282"><a href="#NetworkSlimming-282"><span class="linenos">282</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The network_transform attribute of the training_strategy should be of&quot;</span>
</span><span id="NetworkSlimming-283"><a href="#NetworkSlimming-283"><span class="linenos">283</span></a>                            <span class="s2">&quot; the class NSTransform&quot;</span><span class="p">)</span>
</span><span id="NetworkSlimming-284"><a href="#NetworkSlimming-284"><span class="linenos">284</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span> <span class="o">=</span> <span class="n">pre_training</span>
</span><span id="NetworkSlimming-285"><a href="#NetworkSlimming-285"><span class="linenos">285</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pre_training</span><span class="p">:</span>
</span><span id="NetworkSlimming-286"><a href="#NetworkSlimming-286"><span class="linenos">286</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;If pre_training is True then training_strategy must not be None&quot;</span><span class="p">)</span>
</span><span id="NetworkSlimming-287"><a href="#NetworkSlimming-287"><span class="linenos">287</span></a>
</span><span id="NetworkSlimming-288"><a href="#NetworkSlimming-288"><span class="linenos">288</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="NetworkSlimming-289"><a href="#NetworkSlimming-289"><span class="linenos">289</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="NetworkSlimming-290"><a href="#NetworkSlimming-290"><span class="linenos">290</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Network Slimming.</span>
</span><span id="NetworkSlimming-291"><a href="#NetworkSlimming-291"><span class="linenos">291</span></a>
</span><span id="NetworkSlimming-292"><a href="#NetworkSlimming-292"><span class="linenos">292</span></a><span class="sd">        Parameters</span>
</span><span id="NetworkSlimming-293"><a href="#NetworkSlimming-293"><span class="linenos">293</span></a><span class="sd">        ----------</span>
</span><span id="NetworkSlimming-294"><a href="#NetworkSlimming-294"><span class="linenos">294</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="NetworkSlimming-295"><a href="#NetworkSlimming-295"><span class="linenos">295</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="NetworkSlimming-296"><a href="#NetworkSlimming-296"><span class="linenos">296</span></a><span class="sd">        dataset: Dataset</span>
</span><span id="NetworkSlimming-297"><a href="#NetworkSlimming-297"><span class="linenos">297</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="NetworkSlimming-298"><a href="#NetworkSlimming-298"><span class="linenos">298</span></a>
</span><span id="NetworkSlimming-299"><a href="#NetworkSlimming-299"><span class="linenos">299</span></a><span class="sd">        Returns</span>
</span><span id="NetworkSlimming-300"><a href="#NetworkSlimming-300"><span class="linenos">300</span></a><span class="sd">        ----------</span>
</span><span id="NetworkSlimming-301"><a href="#NetworkSlimming-301"><span class="linenos">301</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="NetworkSlimming-302"><a href="#NetworkSlimming-302"><span class="linenos">302</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="NetworkSlimming-303"><a href="#NetworkSlimming-303"><span class="linenos">303</span></a>
</span><span id="NetworkSlimming-304"><a href="#NetworkSlimming-304"><span class="linenos">304</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NetworkSlimming-305"><a href="#NetworkSlimming-305"><span class="linenos">305</span></a>
</span><span id="NetworkSlimming-306"><a href="#NetworkSlimming-306"><span class="linenos">306</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span><span class="p">:</span>
</span><span id="NetworkSlimming-307"><a href="#NetworkSlimming-307"><span class="linenos">307</span></a>            <span class="n">fine_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span>
</span><span id="NetworkSlimming-308"><a href="#NetworkSlimming-308"><span class="linenos">308</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="NetworkSlimming-309"><a href="#NetworkSlimming-309"><span class="linenos">309</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="NetworkSlimming-310"><a href="#NetworkSlimming-310"><span class="linenos">310</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="NetworkSlimming-311"><a href="#NetworkSlimming-311"><span class="linenos">311</span></a>
</span><span id="NetworkSlimming-312"><a href="#NetworkSlimming-312"><span class="linenos">312</span></a>        <span class="n">pytorch_converter</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchConverter</span><span class="p">()</span>
</span><span id="NetworkSlimming-313"><a href="#NetworkSlimming-313"><span class="linenos">313</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="n">pytorch_converter</span><span class="o">.</span><span class="n">from_neural_network</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</span><span id="NetworkSlimming-314"><a href="#NetworkSlimming-314"><span class="linenos">314</span></a>
</span><span id="NetworkSlimming-315"><a href="#NetworkSlimming-315"><span class="linenos">315</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pruning</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="NetworkSlimming-316"><a href="#NetworkSlimming-316"><span class="linenos">316</span></a>
</span><span id="NetworkSlimming-317"><a href="#NetworkSlimming-317"><span class="linenos">317</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</span><span id="NetworkSlimming-318"><a href="#NetworkSlimming-318"><span class="linenos">318</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="NetworkSlimming-319"><a href="#NetworkSlimming-319"><span class="linenos">319</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">up_to_date</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="NetworkSlimming-320"><a href="#NetworkSlimming-320"><span class="linenos">320</span></a>
</span><span id="NetworkSlimming-321"><a href="#NetworkSlimming-321"><span class="linenos">321</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="NetworkSlimming-322"><a href="#NetworkSlimming-322"><span class="linenos">322</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="NetworkSlimming-323"><a href="#NetworkSlimming-323"><span class="linenos">323</span></a>
</span><span id="NetworkSlimming-324"><a href="#NetworkSlimming-324"><span class="linenos">324</span></a>        <span class="k">return</span> <span class="n">network</span>
</span><span id="NetworkSlimming-325"><a href="#NetworkSlimming-325"><span class="linenos">325</span></a>
</span><span id="NetworkSlimming-326"><a href="#NetworkSlimming-326"><span class="linenos">326</span></a>    <span class="k">def</span> <span class="nf">__pruning</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchNetwork</span><span class="p">):</span>
</span><span id="NetworkSlimming-327"><a href="#NetworkSlimming-327"><span class="linenos">327</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="NetworkSlimming-328"><a href="#NetworkSlimming-328"><span class="linenos">328</span></a><span class="sd">        Procedure for the pruning of the neurons of the PyTorchNetwork passed as an argument.</span>
</span><span id="NetworkSlimming-329"><a href="#NetworkSlimming-329"><span class="linenos">329</span></a>
</span><span id="NetworkSlimming-330"><a href="#NetworkSlimming-330"><span class="linenos">330</span></a><span class="sd">        Parameters</span>
</span><span id="NetworkSlimming-331"><a href="#NetworkSlimming-331"><span class="linenos">331</span></a><span class="sd">        ----------</span>
</span><span id="NetworkSlimming-332"><a href="#NetworkSlimming-332"><span class="linenos">332</span></a><span class="sd">        net : PyTorchNetwork</span>
</span><span id="NetworkSlimming-333"><a href="#NetworkSlimming-333"><span class="linenos">333</span></a><span class="sd">            The PyTorchNetwork to prune.</span>
</span><span id="NetworkSlimming-334"><a href="#NetworkSlimming-334"><span class="linenos">334</span></a>
</span><span id="NetworkSlimming-335"><a href="#NetworkSlimming-335"><span class="linenos">335</span></a><span class="sd">        Returns</span>
</span><span id="NetworkSlimming-336"><a href="#NetworkSlimming-336"><span class="linenos">336</span></a><span class="sd">        ----------</span>
</span><span id="NetworkSlimming-337"><a href="#NetworkSlimming-337"><span class="linenos">337</span></a><span class="sd">        PyTorchNetwork</span>
</span><span id="NetworkSlimming-338"><a href="#NetworkSlimming-338"><span class="linenos">338</span></a><span class="sd">            The PyTorchNetwork resulting from the application of the pure pruning procedure.</span>
</span><span id="NetworkSlimming-339"><a href="#NetworkSlimming-339"><span class="linenos">339</span></a>
</span><span id="NetworkSlimming-340"><a href="#NetworkSlimming-340"><span class="linenos">340</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NetworkSlimming-341"><a href="#NetworkSlimming-341"><span class="linenos">341</span></a>
</span><span id="NetworkSlimming-342"><a href="#NetworkSlimming-342"><span class="linenos">342</span></a>        <span class="c1"># We transfer the internal pytorch model to the CPU for the pruning procedure.</span>
</span><span id="NetworkSlimming-343"><a href="#NetworkSlimming-343"><span class="linenos">343</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span><span id="NetworkSlimming-344"><a href="#NetworkSlimming-344"><span class="linenos">344</span></a>
</span><span id="NetworkSlimming-345"><a href="#NetworkSlimming-345"><span class="linenos">345</span></a>        <span class="c1"># We compute the total number of weights in the batch normalization layers (which, for fully connected networks,</span>
</span><span id="NetworkSlimming-346"><a href="#NetworkSlimming-346"><span class="linenos">346</span></a>        <span class="c1"># is equal to the number of neurons in the corresponding fully-connected layer).</span>
</span><span id="NetworkSlimming-347"><a href="#NetworkSlimming-347"><span class="linenos">347</span></a>        <span class="n">num_bn_weights</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="NetworkSlimming-348"><a href="#NetworkSlimming-348"><span class="linenos">348</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="NetworkSlimming-349"><a href="#NetworkSlimming-349"><span class="linenos">349</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="NetworkSlimming-350"><a href="#NetworkSlimming-350"><span class="linenos">350</span></a>                <span class="n">num_bn_weights</span> <span class="o">+=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="NetworkSlimming-351"><a href="#NetworkSlimming-351"><span class="linenos">351</span></a>
</span><span id="NetworkSlimming-352"><a href="#NetworkSlimming-352"><span class="linenos">352</span></a>        <span class="c1"># We copy all the absolute values of the batch norm weights in a new tensor and we sort in ascending order</span>
</span><span id="NetworkSlimming-353"><a href="#NetworkSlimming-353"><span class="linenos">353</span></a>        <span class="n">bn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_bn_weights</span><span class="p">)</span>
</span><span id="NetworkSlimming-354"><a href="#NetworkSlimming-354"><span class="linenos">354</span></a>        <span class="n">bn_weights_index</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="NetworkSlimming-355"><a href="#NetworkSlimming-355"><span class="linenos">355</span></a>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
</span><span id="NetworkSlimming-356"><a href="#NetworkSlimming-356"><span class="linenos">356</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="NetworkSlimming-357"><a href="#NetworkSlimming-357"><span class="linenos">357</span></a>                <span class="n">size</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="NetworkSlimming-358"><a href="#NetworkSlimming-358"><span class="linenos">358</span></a>                <span class="n">bn_weights</span><span class="p">[</span><span class="n">bn_weights_index</span><span class="p">:(</span><span class="n">bn_weights_index</span> <span class="o">+</span> <span class="n">size</span><span class="p">)]</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-359"><a href="#NetworkSlimming-359"><span class="linenos">359</span></a>                <span class="n">bn_weights_index</span> <span class="o">+=</span> <span class="n">size</span>
</span><span id="NetworkSlimming-360"><a href="#NetworkSlimming-360"><span class="linenos">360</span></a>
</span><span id="NetworkSlimming-361"><a href="#NetworkSlimming-361"><span class="linenos">361</span></a>        <span class="n">ordered_bn_weights</span><span class="p">,</span> <span class="n">ordered_bn_indexes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">bn_weights</span><span class="p">)</span>
</span><span id="NetworkSlimming-362"><a href="#NetworkSlimming-362"><span class="linenos">362</span></a>
</span><span id="NetworkSlimming-363"><a href="#NetworkSlimming-363"><span class="linenos">363</span></a>        <span class="c1"># We determine the number of neurons we need to remove, given the sparsity rate.</span>
</span><span id="NetworkSlimming-364"><a href="#NetworkSlimming-364"><span class="linenos">364</span></a>        <span class="n">threshold_index</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">num_bn_weights</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span><span class="p">)</span>
</span><span id="NetworkSlimming-365"><a href="#NetworkSlimming-365"><span class="linenos">365</span></a>
</span><span id="NetworkSlimming-366"><a href="#NetworkSlimming-366"><span class="linenos">366</span></a>        <span class="c1"># We select the batch norm weight absolute value we will use as threshold value given the threshold index.</span>
</span><span id="NetworkSlimming-367"><a href="#NetworkSlimming-367"><span class="linenos">367</span></a>        <span class="n">threshold_value</span> <span class="o">=</span> <span class="n">ordered_bn_weights</span><span class="p">[</span><span class="n">threshold_index</span><span class="p">]</span>
</span><span id="NetworkSlimming-368"><a href="#NetworkSlimming-368"><span class="linenos">368</span></a>
</span><span id="NetworkSlimming-369"><a href="#NetworkSlimming-369"><span class="linenos">369</span></a>        <span class="c1"># We now need to create a new network with the correct number of neurons in the different layers.</span>
</span><span id="NetworkSlimming-370"><a href="#NetworkSlimming-370"><span class="linenos">370</span></a>        <span class="c1"># To do so we assume that in the network after a linear layer there is always a batch norm layer.</span>
</span><span id="NetworkSlimming-371"><a href="#NetworkSlimming-371"><span class="linenos">371</span></a>
</span><span id="NetworkSlimming-372"><a href="#NetworkSlimming-372"><span class="linenos">372</span></a>        <span class="n">new_layers</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="NetworkSlimming-373"><a href="#NetworkSlimming-373"><span class="linenos">373</span></a>        <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="NetworkSlimming-374"><a href="#NetworkSlimming-374"><span class="linenos">374</span></a>        <span class="n">old_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span><span class="o">.</span><span class="n">modules</span><span class="p">()]</span>
</span><span id="NetworkSlimming-375"><a href="#NetworkSlimming-375"><span class="linenos">375</span></a>        <span class="n">orig_seq</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NetworkSlimming-376"><a href="#NetworkSlimming-376"><span class="linenos">376</span></a>        <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">old_layers</span><span class="p">)</span>
</span><span id="NetworkSlimming-377"><a href="#NetworkSlimming-377"><span class="linenos">377</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
</span><span id="NetworkSlimming-378"><a href="#NetworkSlimming-378"><span class="linenos">378</span></a>
</span><span id="NetworkSlimming-379"><a href="#NetworkSlimming-379"><span class="linenos">379</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="NetworkSlimming-380"><a href="#NetworkSlimming-380"><span class="linenos">380</span></a>
</span><span id="NetworkSlimming-381"><a href="#NetworkSlimming-381"><span class="linenos">381</span></a>                <span class="c1"># In this case we are considering the last layer of the network (which we assume to be a linear layer),</span>
</span><span id="NetworkSlimming-382"><a href="#NetworkSlimming-382"><span class="linenos">382</span></a>                <span class="c1"># therefore the number of output of the new layer will be equal to the one of the old layer.</span>
</span><span id="NetworkSlimming-383"><a href="#NetworkSlimming-383"><span class="linenos">383</span></a>
</span><span id="NetworkSlimming-384"><a href="#NetworkSlimming-384"><span class="linenos">384</span></a>                <span class="n">previous_nonzero_indexes</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NetworkSlimming-385"><a href="#NetworkSlimming-385"><span class="linenos">385</span></a>
</span><span id="NetworkSlimming-386"><a href="#NetworkSlimming-386"><span class="linenos">386</span></a>                <span class="c1"># If the old linear layer had bias then also the new linear layer has them.</span>
</span><span id="NetworkSlimming-387"><a href="#NetworkSlimming-387"><span class="linenos">387</span></a>                <span class="k">if</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NetworkSlimming-388"><a href="#NetworkSlimming-388"><span class="linenos">388</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="NetworkSlimming-389"><a href="#NetworkSlimming-389"><span class="linenos">389</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="NetworkSlimming-390"><a href="#NetworkSlimming-390"><span class="linenos">390</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="NetworkSlimming-391"><a href="#NetworkSlimming-391"><span class="linenos">391</span></a>
</span><span id="NetworkSlimming-392"><a href="#NetworkSlimming-392"><span class="linenos">392</span></a>                <span class="c1"># The number of input features for the new linear layer is equal to the number of non-zero elements in</span>
</span><span id="NetworkSlimming-393"><a href="#NetworkSlimming-393"><span class="linenos">393</span></a>                <span class="c1"># the mask of the previous layer.</span>
</span><span id="NetworkSlimming-394"><a href="#NetworkSlimming-394"><span class="linenos">394</span></a>                <span class="n">num_in_features</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span><span id="NetworkSlimming-395"><a href="#NetworkSlimming-395"><span class="linenos">395</span></a>
</span><span id="NetworkSlimming-396"><a href="#NetworkSlimming-396"><span class="linenos">396</span></a>                <span class="c1"># We create the new linear layer with the correct architecture.</span>
</span><span id="NetworkSlimming-397"><a href="#NetworkSlimming-397"><span class="linenos">397</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-398"><a href="#NetworkSlimming-398"><span class="linenos">398</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_in_features</span>
</span><span id="NetworkSlimming-399"><a href="#NetworkSlimming-399"><span class="linenos">399</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-400"><a href="#NetworkSlimming-400"><span class="linenos">400</span></a>                <span class="n">new_linear_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span>
</span><span id="NetworkSlimming-401"><a href="#NetworkSlimming-401"><span class="linenos">401</span></a>                                              <span class="n">num_in_features</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">)</span>
</span><span id="NetworkSlimming-402"><a href="#NetworkSlimming-402"><span class="linenos">402</span></a>
</span><span id="NetworkSlimming-403"><a href="#NetworkSlimming-403"><span class="linenos">403</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons.</span>
</span><span id="NetworkSlimming-404"><a href="#NetworkSlimming-404"><span class="linenos">404</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">previous_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-405"><a href="#NetworkSlimming-405"><span class="linenos">405</span></a>
</span><span id="NetworkSlimming-406"><a href="#NetworkSlimming-406"><span class="linenos">406</span></a>                <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
</span><span id="NetworkSlimming-407"><a href="#NetworkSlimming-407"><span class="linenos">407</span></a>                    <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span>
</span><span id="NetworkSlimming-408"><a href="#NetworkSlimming-408"><span class="linenos">408</span></a>
</span><span id="NetworkSlimming-409"><a href="#NetworkSlimming-409"><span class="linenos">409</span></a>                <span class="c1"># We save the new linear layer.</span>
</span><span id="NetworkSlimming-410"><a href="#NetworkSlimming-410"><span class="linenos">410</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_linear_layer</span><span class="p">)</span>
</span><span id="NetworkSlimming-411"><a href="#NetworkSlimming-411"><span class="linenos">411</span></a>
</span><span id="NetworkSlimming-412"><a href="#NetworkSlimming-412"><span class="linenos">412</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="NetworkSlimming-413"><a href="#NetworkSlimming-413"><span class="linenos">413</span></a>
</span><span id="NetworkSlimming-414"><a href="#NetworkSlimming-414"><span class="linenos">414</span></a>                <span class="c1"># If the layer old_layers[i] is the first linear layer then the previous layer mask corrspond to the</span>
</span><span id="NetworkSlimming-415"><a href="#NetworkSlimming-415"><span class="linenos">415</span></a>                <span class="c1"># complete input.</span>
</span><span id="NetworkSlimming-416"><a href="#NetworkSlimming-416"><span class="linenos">416</span></a>                <span class="k">if</span> <span class="n">previous_layer_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NetworkSlimming-417"><a href="#NetworkSlimming-417"><span class="linenos">417</span></a>                    <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
</span><span id="NetworkSlimming-418"><a href="#NetworkSlimming-418"><span class="linenos">418</span></a>
</span><span id="NetworkSlimming-419"><a href="#NetworkSlimming-419"><span class="linenos">419</span></a>                <span class="c1"># We compute the mask corresponding to the batch normalization layer.</span>
</span><span id="NetworkSlimming-420"><a href="#NetworkSlimming-420"><span class="linenos">420</span></a>                <span class="n">layer_mask</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">threshold_value</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="NetworkSlimming-421"><a href="#NetworkSlimming-421"><span class="linenos">421</span></a>                <span class="n">new_neuron_number</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span><span id="NetworkSlimming-422"><a href="#NetworkSlimming-422"><span class="linenos">422</span></a>
</span><span id="NetworkSlimming-423"><a href="#NetworkSlimming-423"><span class="linenos">423</span></a>                <span class="c1"># We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.</span>
</span><span id="NetworkSlimming-424"><a href="#NetworkSlimming-424"><span class="linenos">424</span></a>                <span class="n">current_nonzero_indexes</span> <span class="o">=</span> <span class="n">layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NetworkSlimming-425"><a href="#NetworkSlimming-425"><span class="linenos">425</span></a>                <span class="n">previous_nonzero_indexes</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NetworkSlimming-426"><a href="#NetworkSlimming-426"><span class="linenos">426</span></a>
</span><span id="NetworkSlimming-427"><a href="#NetworkSlimming-427"><span class="linenos">427</span></a>                <span class="c1"># We create the new batch norm layer with the new neuron number.</span>
</span><span id="NetworkSlimming-428"><a href="#NetworkSlimming-428"><span class="linenos">428</span></a>
</span><span id="NetworkSlimming-429"><a href="#NetworkSlimming-429"><span class="linenos">429</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-430"><a href="#NetworkSlimming-430"><span class="linenos">430</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_neuron_number</span>
</span><span id="NetworkSlimming-431"><a href="#NetworkSlimming-431"><span class="linenos">431</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-432"><a href="#NetworkSlimming-432"><span class="linenos">432</span></a>                <span class="n">new_bn_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">new_neuron_number</span><span class="p">,</span>
</span><span id="NetworkSlimming-433"><a href="#NetworkSlimming-433"><span class="linenos">433</span></a>                                               <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span>
</span><span id="NetworkSlimming-434"><a href="#NetworkSlimming-434"><span class="linenos">434</span></a>                                               <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">affine</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">track_running_stats</span><span class="p">)</span>
</span><span id="NetworkSlimming-435"><a href="#NetworkSlimming-435"><span class="linenos">435</span></a>
</span><span id="NetworkSlimming-436"><a href="#NetworkSlimming-436"><span class="linenos">436</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons from the old batch norm layer</span>
</span><span id="NetworkSlimming-437"><a href="#NetworkSlimming-437"><span class="linenos">437</span></a>                <span class="c1"># to the new one. They are identified by the indexes in current_nonzero_indexes.</span>
</span><span id="NetworkSlimming-438"><a href="#NetworkSlimming-438"><span class="linenos">438</span></a>
</span><span id="NetworkSlimming-439"><a href="#NetworkSlimming-439"><span class="linenos">439</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-440"><a href="#NetworkSlimming-440"><span class="linenos">440</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-441"><a href="#NetworkSlimming-441"><span class="linenos">441</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">running_mean</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-442"><a href="#NetworkSlimming-442"><span class="linenos">442</span></a>                <span class="n">new_bn_layer</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">running_var</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-443"><a href="#NetworkSlimming-443"><span class="linenos">443</span></a>
</span><span id="NetworkSlimming-444"><a href="#NetworkSlimming-444"><span class="linenos">444</span></a>                <span class="c1"># If the old linear layer had bias then also the new linear layer has them.</span>
</span><span id="NetworkSlimming-445"><a href="#NetworkSlimming-445"><span class="linenos">445</span></a>                <span class="k">if</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NetworkSlimming-446"><a href="#NetworkSlimming-446"><span class="linenos">446</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="NetworkSlimming-447"><a href="#NetworkSlimming-447"><span class="linenos">447</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="NetworkSlimming-448"><a href="#NetworkSlimming-448"><span class="linenos">448</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="NetworkSlimming-449"><a href="#NetworkSlimming-449"><span class="linenos">449</span></a>
</span><span id="NetworkSlimming-450"><a href="#NetworkSlimming-450"><span class="linenos">450</span></a>                <span class="c1"># The number of input features for the new linear layer is equal to the number of non-zero elements in</span>
</span><span id="NetworkSlimming-451"><a href="#NetworkSlimming-451"><span class="linenos">451</span></a>                <span class="c1"># the mask of the previous layer.</span>
</span><span id="NetworkSlimming-452"><a href="#NetworkSlimming-452"><span class="linenos">452</span></a>                <span class="n">num_in_features</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span><span id="NetworkSlimming-453"><a href="#NetworkSlimming-453"><span class="linenos">453</span></a>
</span><span id="NetworkSlimming-454"><a href="#NetworkSlimming-454"><span class="linenos">454</span></a>                <span class="c1"># We create the new linear layer with the correct architecture.</span>
</span><span id="NetworkSlimming-455"><a href="#NetworkSlimming-455"><span class="linenos">455</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-456"><a href="#NetworkSlimming-456"><span class="linenos">456</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_in_features</span>
</span><span id="NetworkSlimming-457"><a href="#NetworkSlimming-457"><span class="linenos">457</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-458"><a href="#NetworkSlimming-458"><span class="linenos">458</span></a>                <span class="n">new_out_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-459"><a href="#NetworkSlimming-459"><span class="linenos">459</span></a>                <span class="n">new_out_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_neuron_number</span>
</span><span id="NetworkSlimming-460"><a href="#NetworkSlimming-460"><span class="linenos">460</span></a>                <span class="n">new_out_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_out_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-461"><a href="#NetworkSlimming-461"><span class="linenos">461</span></a>                <span class="n">new_linear_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">new_out_dim</span><span class="p">,</span>
</span><span id="NetworkSlimming-462"><a href="#NetworkSlimming-462"><span class="linenos">462</span></a>                                              <span class="n">num_in_features</span><span class="p">,</span> <span class="n">new_neuron_number</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">)</span>
</span><span id="NetworkSlimming-463"><a href="#NetworkSlimming-463"><span class="linenos">463</span></a>
</span><span id="NetworkSlimming-464"><a href="#NetworkSlimming-464"><span class="linenos">464</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons.</span>
</span><span id="NetworkSlimming-465"><a href="#NetworkSlimming-465"><span class="linenos">465</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-466"><a href="#NetworkSlimming-466"><span class="linenos">466</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">previous_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-467"><a href="#NetworkSlimming-467"><span class="linenos">467</span></a>
</span><span id="NetworkSlimming-468"><a href="#NetworkSlimming-468"><span class="linenos">468</span></a>                <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
</span><span id="NetworkSlimming-469"><a href="#NetworkSlimming-469"><span class="linenos">469</span></a>                    <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span>
</span><span id="NetworkSlimming-470"><a href="#NetworkSlimming-470"><span class="linenos">470</span></a>
</span><span id="NetworkSlimming-471"><a href="#NetworkSlimming-471"><span class="linenos">471</span></a>                <span class="c1"># We save the new layers in the order in which they should be in our sequential model: first the linear</span>
</span><span id="NetworkSlimming-472"><a href="#NetworkSlimming-472"><span class="linenos">472</span></a>                <span class="c1"># layer and then the batch norm layer.</span>
</span><span id="NetworkSlimming-473"><a href="#NetworkSlimming-473"><span class="linenos">473</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_linear_layer</span><span class="p">)</span>
</span><span id="NetworkSlimming-474"><a href="#NetworkSlimming-474"><span class="linenos">474</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_bn_layer</span><span class="p">)</span>
</span><span id="NetworkSlimming-475"><a href="#NetworkSlimming-475"><span class="linenos">475</span></a>
</span><span id="NetworkSlimming-476"><a href="#NetworkSlimming-476"><span class="linenos">476</span></a>                <span class="c1"># We update the value of previous_layer_mask with the current mask.</span>
</span><span id="NetworkSlimming-477"><a href="#NetworkSlimming-477"><span class="linenos">477</span></a>                <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="n">layer_mask</span>
</span><span id="NetworkSlimming-478"><a href="#NetworkSlimming-478"><span class="linenos">478</span></a>
</span><span id="NetworkSlimming-479"><a href="#NetworkSlimming-479"><span class="linenos">479</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
</span><span id="NetworkSlimming-480"><a href="#NetworkSlimming-480"><span class="linenos">480</span></a>
</span><span id="NetworkSlimming-481"><a href="#NetworkSlimming-481"><span class="linenos">481</span></a>                <span class="c1"># If the layer old_layers[i] is the first linear layer then the previous layer mask correspond to the</span>
</span><span id="NetworkSlimming-482"><a href="#NetworkSlimming-482"><span class="linenos">482</span></a>                <span class="c1"># complete input.</span>
</span><span id="NetworkSlimming-483"><a href="#NetworkSlimming-483"><span class="linenos">483</span></a>                <span class="k">if</span> <span class="n">previous_layer_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NetworkSlimming-484"><a href="#NetworkSlimming-484"><span class="linenos">484</span></a>                    <span class="n">previous_layer_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
</span><span id="NetworkSlimming-485"><a href="#NetworkSlimming-485"><span class="linenos">485</span></a>
</span><span id="NetworkSlimming-486"><a href="#NetworkSlimming-486"><span class="linenos">486</span></a>                <span class="c1"># If the linear layer is not followed by a batch normalization layer then it will not be neuron pruned,</span>
</span><span id="NetworkSlimming-487"><a href="#NetworkSlimming-487"><span class="linenos">487</span></a>                <span class="c1"># therefore the layer_mask will be equals to the number of output features of the old layer.</span>
</span><span id="NetworkSlimming-488"><a href="#NetworkSlimming-488"><span class="linenos">488</span></a>                <span class="n">layer_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_features</span><span class="p">)</span>
</span><span id="NetworkSlimming-489"><a href="#NetworkSlimming-489"><span class="linenos">489</span></a>
</span><span id="NetworkSlimming-490"><a href="#NetworkSlimming-490"><span class="linenos">490</span></a>                <span class="c1"># We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.</span>
</span><span id="NetworkSlimming-491"><a href="#NetworkSlimming-491"><span class="linenos">491</span></a>                <span class="n">current_nonzero_indexes</span> <span class="o">=</span> <span class="n">layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NetworkSlimming-492"><a href="#NetworkSlimming-492"><span class="linenos">492</span></a>                <span class="n">previous_nonzero_indexes</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="NetworkSlimming-493"><a href="#NetworkSlimming-493"><span class="linenos">493</span></a>
</span><span id="NetworkSlimming-494"><a href="#NetworkSlimming-494"><span class="linenos">494</span></a>                <span class="k">if</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="NetworkSlimming-495"><a href="#NetworkSlimming-495"><span class="linenos">495</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="NetworkSlimming-496"><a href="#NetworkSlimming-496"><span class="linenos">496</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="NetworkSlimming-497"><a href="#NetworkSlimming-497"><span class="linenos">497</span></a>                    <span class="n">has_bias</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="NetworkSlimming-498"><a href="#NetworkSlimming-498"><span class="linenos">498</span></a>
</span><span id="NetworkSlimming-499"><a href="#NetworkSlimming-499"><span class="linenos">499</span></a>                <span class="c1"># The number of input features for the new linear layer is equal to the number of non-zero elements in</span>
</span><span id="NetworkSlimming-500"><a href="#NetworkSlimming-500"><span class="linenos">500</span></a>                <span class="c1"># the mask of the previous layer.</span>
</span><span id="NetworkSlimming-501"><a href="#NetworkSlimming-501"><span class="linenos">501</span></a>                <span class="n">num_in_features</span> <span class="o">=</span> <span class="n">previous_layer_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span><span id="NetworkSlimming-502"><a href="#NetworkSlimming-502"><span class="linenos">502</span></a>
</span><span id="NetworkSlimming-503"><a href="#NetworkSlimming-503"><span class="linenos">503</span></a>                <span class="c1"># We create the new linear layer with the correct architecture.</span>
</span><span id="NetworkSlimming-504"><a href="#NetworkSlimming-504"><span class="linenos">504</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-505"><a href="#NetworkSlimming-505"><span class="linenos">505</span></a>                <span class="n">new_in_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_in_features</span>
</span><span id="NetworkSlimming-506"><a href="#NetworkSlimming-506"><span class="linenos">506</span></a>                <span class="n">new_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_in_dim</span><span class="p">)</span>
</span><span id="NetworkSlimming-507"><a href="#NetworkSlimming-507"><span class="linenos">507</span></a>                <span class="n">new_linear_layer</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">new_in_dim</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span>
</span><span id="NetworkSlimming-508"><a href="#NetworkSlimming-508"><span class="linenos">508</span></a>                                              <span class="n">num_in_features</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">)</span>
</span><span id="NetworkSlimming-509"><a href="#NetworkSlimming-509"><span class="linenos">509</span></a>
</span><span id="NetworkSlimming-510"><a href="#NetworkSlimming-510"><span class="linenos">510</span></a>                <span class="c1"># We copy the parameters corresponding to the still existing neurons.</span>
</span><span id="NetworkSlimming-511"><a href="#NetworkSlimming-511"><span class="linenos">511</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-512"><a href="#NetworkSlimming-512"><span class="linenos">512</span></a>                <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">previous_nonzero_indexes</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="NetworkSlimming-513"><a href="#NetworkSlimming-513"><span class="linenos">513</span></a>
</span><span id="NetworkSlimming-514"><a href="#NetworkSlimming-514"><span class="linenos">514</span></a>                <span class="k">if</span> <span class="n">has_bias</span><span class="p">:</span>
</span><span id="NetworkSlimming-515"><a href="#NetworkSlimming-515"><span class="linenos">515</span></a>                    <span class="n">new_linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">current_nonzero_indexes</span><span class="p">]</span>
</span><span id="NetworkSlimming-516"><a href="#NetworkSlimming-516"><span class="linenos">516</span></a>
</span><span id="NetworkSlimming-517"><a href="#NetworkSlimming-517"><span class="linenos">517</span></a>                <span class="c1"># We save the new layers in the order in which they should be in our sequential model: first the linear</span>
</span><span id="NetworkSlimming-518"><a href="#NetworkSlimming-518"><span class="linenos">518</span></a>                <span class="c1"># layer and then the batch norm layer.</span>
</span><span id="NetworkSlimming-519"><a href="#NetworkSlimming-519"><span class="linenos">519</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_linear_layer</span><span class="p">)</span>
</span><span id="NetworkSlimming-520"><a href="#NetworkSlimming-520"><span class="linenos">520</span></a>
</span><span id="NetworkSlimming-521"><a href="#NetworkSlimming-521"><span class="linenos">521</span></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ptl</span><span class="o">.</span><span class="n">ReLU</span><span class="p">):</span>
</span><span id="NetworkSlimming-522"><a href="#NetworkSlimming-522"><span class="linenos">522</span></a>                <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ptl</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">old_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">out_dim</span><span class="p">))</span>
</span><span id="NetworkSlimming-523"><a href="#NetworkSlimming-523"><span class="linenos">523</span></a>
</span><span id="NetworkSlimming-524"><a href="#NetworkSlimming-524"><span class="linenos">524</span></a>        <span class="n">pruned_network</span> <span class="o">=</span> <span class="n">ptl</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">orig_seq</span><span class="o">.</span><span class="n">identifier</span><span class="p">,</span> <span class="n">orig_seq</span><span class="o">.</span><span class="n">input_id</span><span class="p">,</span> <span class="n">new_layers</span><span class="p">)</span>
</span><span id="NetworkSlimming-525"><a href="#NetworkSlimming-525"><span class="linenos">525</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">pytorch_network</span> <span class="o">=</span> <span class="n">pruned_network</span>
</span><span id="NetworkSlimming-526"><a href="#NetworkSlimming-526"><span class="linenos">526</span></a>        <span class="n">net</span><span class="o">.</span><span class="n">identifier</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">identifier</span> <span class="o">+</span> <span class="s1">&#39;_pruned&#39;</span>
</span><span id="NetworkSlimming-527"><a href="#NetworkSlimming-527"><span class="linenos">527</span></a>        <span class="k">return</span> <span class="n">net</span>
</span></pre></div>


            <div class="docstring"><p>A concrete class used to represent the network slimming pruning strategy.
This kind of pruning select the least important neurons of the neural network
of interest and eliminates them. It needs a batch normalization layer following each layer
which should be pruned. We assume that the activation function is always applied after the batch
normalization layer. It support only networks with linear and batchnorm layers with vectorial inputs
We refer to <a href="https://arxiv.org/abs/1708.06519">https://arxiv.org/abs/1708.06519</a> for theoretical details on the strategy.</p>

<h2 id="attributes">Attributes</h2>

<p>sparsity_rate : float
    It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.
training_strategy : PytorchTraining
    The training strategy to use for pre-training and/or fine-tuning. NB: Its network_transform parameter must
    be of the class NSTransform.
pre_training : bool
    Flag to indicate if the network need to be pre-trained.</p>

<h2 id="methods">Methods</h2>

<p>prune(NeuralNetwork, Dataset)
    Prune the neural network of interest using the pruning strategy Network Slimming.</p>
</div>


                            <div id="NetworkSlimming.__init__" class="classattr">
                                        <input id="NetworkSlimming.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">NetworkSlimming</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span>,</span><span class="param">	<span class="n">training_strategy</span><span class="p">:</span> <span class="n"><a href="training.html#PytorchTraining">pynever.strategies.training.PytorchTraining</a></span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="NetworkSlimming.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NetworkSlimming.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NetworkSlimming.__init__-276"><a href="#NetworkSlimming.__init__-276"><span class="linenos">276</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">training_strategy</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">PytorchTraining</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="NetworkSlimming.__init__-277"><a href="#NetworkSlimming.__init__-277"><span class="linenos">277</span></a>                 <span class="n">pre_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="NetworkSlimming.__init__-278"><a href="#NetworkSlimming.__init__-278"><span class="linenos">278</span></a>
</span><span id="NetworkSlimming.__init__-279"><a href="#NetworkSlimming.__init__-279"><span class="linenos">279</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_rate</span> <span class="o">=</span> <span class="n">sparsity_rate</span>
</span><span id="NetworkSlimming.__init__-280"><a href="#NetworkSlimming.__init__-280"><span class="linenos">280</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="o">=</span> <span class="n">training_strategy</span>
</span><span id="NetworkSlimming.__init__-281"><a href="#NetworkSlimming.__init__-281"><span class="linenos">281</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="p">,</span> <span class="n">NSTransform</span><span class="p">):</span>
</span><span id="NetworkSlimming.__init__-282"><a href="#NetworkSlimming.__init__-282"><span class="linenos">282</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;The network_transform attribute of the training_strategy should be of&quot;</span>
</span><span id="NetworkSlimming.__init__-283"><a href="#NetworkSlimming.__init__-283"><span class="linenos">283</span></a>                            <span class="s2">&quot; the class NSTransform&quot;</span><span class="p">)</span>
</span><span id="NetworkSlimming.__init__-284"><a href="#NetworkSlimming.__init__-284"><span class="linenos">284</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span> <span class="o">=</span> <span class="n">pre_training</span>
</span><span id="NetworkSlimming.__init__-285"><a href="#NetworkSlimming.__init__-285"><span class="linenos">285</span></a>        <span class="k">if</span> <span class="n">training_strategy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pre_training</span><span class="p">:</span>
</span><span id="NetworkSlimming.__init__-286"><a href="#NetworkSlimming.__init__-286"><span class="linenos">286</span></a>            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;If pre_training is True then training_strategy must not be None&quot;</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="NetworkSlimming.sparsity_rate" class="classattr">
                                <div class="attr variable">
            <span class="name">sparsity_rate</span>

        
    </div>
    <a class="headerlink" href="#NetworkSlimming.sparsity_rate"></a>
    
    

                            </div>
                            <div id="NetworkSlimming.training_strategy" class="classattr">
                                <div class="attr variable">
            <span class="name">training_strategy</span>

        
    </div>
    <a class="headerlink" href="#NetworkSlimming.training_strategy"></a>
    
    

                            </div>
                            <div id="NetworkSlimming.pre_training" class="classattr">
                                <div class="attr variable">
            <span class="name">pre_training</span>

        
    </div>
    <a class="headerlink" href="#NetworkSlimming.pre_training"></a>
    
    

                            </div>
                            <div id="NetworkSlimming.prune" class="classattr">
                                        <input id="NetworkSlimming.prune-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">prune</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">network</span><span class="p">:</span> <span class="n"><a href="../networks.html#NeuralNetwork">pynever.networks.NeuralNetwork</a></span>,</span><span class="param">	<span class="n">dataset</span><span class="p">:</span> <span class="n"><a href="../datasets.html#Dataset">pynever.datasets.Dataset</a></span></span><span class="return-annotation">) -> <span class="n"><a href="../networks.html#NeuralNetwork">pynever.networks.NeuralNetwork</a></span>:</span></span>

                <label class="view-source-button" for="NetworkSlimming.prune-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NetworkSlimming.prune"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NetworkSlimming.prune-288"><a href="#NetworkSlimming.prune-288"><span class="linenos">288</span></a>    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">:</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">networks</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">:</span>
</span><span id="NetworkSlimming.prune-289"><a href="#NetworkSlimming.prune-289"><span class="linenos">289</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="NetworkSlimming.prune-290"><a href="#NetworkSlimming.prune-290"><span class="linenos">290</span></a><span class="sd">        Prune the neural network of interest using the pruning strategy Network Slimming.</span>
</span><span id="NetworkSlimming.prune-291"><a href="#NetworkSlimming.prune-291"><span class="linenos">291</span></a>
</span><span id="NetworkSlimming.prune-292"><a href="#NetworkSlimming.prune-292"><span class="linenos">292</span></a><span class="sd">        Parameters</span>
</span><span id="NetworkSlimming.prune-293"><a href="#NetworkSlimming.prune-293"><span class="linenos">293</span></a><span class="sd">        ----------</span>
</span><span id="NetworkSlimming.prune-294"><a href="#NetworkSlimming.prune-294"><span class="linenos">294</span></a><span class="sd">        network : NeuralNetwork</span>
</span><span id="NetworkSlimming.prune-295"><a href="#NetworkSlimming.prune-295"><span class="linenos">295</span></a><span class="sd">            The neural network to prune.</span>
</span><span id="NetworkSlimming.prune-296"><a href="#NetworkSlimming.prune-296"><span class="linenos">296</span></a><span class="sd">        dataset: Dataset</span>
</span><span id="NetworkSlimming.prune-297"><a href="#NetworkSlimming.prune-297"><span class="linenos">297</span></a><span class="sd">            The dataset to use for the pre-training and fine-tuning procedure.</span>
</span><span id="NetworkSlimming.prune-298"><a href="#NetworkSlimming.prune-298"><span class="linenos">298</span></a>
</span><span id="NetworkSlimming.prune-299"><a href="#NetworkSlimming.prune-299"><span class="linenos">299</span></a><span class="sd">        Returns</span>
</span><span id="NetworkSlimming.prune-300"><a href="#NetworkSlimming.prune-300"><span class="linenos">300</span></a><span class="sd">        ----------</span>
</span><span id="NetworkSlimming.prune-301"><a href="#NetworkSlimming.prune-301"><span class="linenos">301</span></a><span class="sd">        NeuralNetwork</span>
</span><span id="NetworkSlimming.prune-302"><a href="#NetworkSlimming.prune-302"><span class="linenos">302</span></a><span class="sd">            The Neural Network resulting from the application of the pruning strategy to the original network.</span>
</span><span id="NetworkSlimming.prune-303"><a href="#NetworkSlimming.prune-303"><span class="linenos">303</span></a>
</span><span id="NetworkSlimming.prune-304"><a href="#NetworkSlimming.prune-304"><span class="linenos">304</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="NetworkSlimming.prune-305"><a href="#NetworkSlimming.prune-305"><span class="linenos">305</span></a>
</span><span id="NetworkSlimming.prune-306"><a href="#NetworkSlimming.prune-306"><span class="linenos">306</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_training</span><span class="p">:</span>
</span><span id="NetworkSlimming.prune-307"><a href="#NetworkSlimming.prune-307"><span class="linenos">307</span></a>            <span class="n">fine_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span>
</span><span id="NetworkSlimming.prune-308"><a href="#NetworkSlimming.prune-308"><span class="linenos">308</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="NetworkSlimming.prune-309"><a href="#NetworkSlimming.prune-309"><span class="linenos">309</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="NetworkSlimming.prune-310"><a href="#NetworkSlimming.prune-310"><span class="linenos">310</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">fine_tuning</span> <span class="o">=</span> <span class="n">fine_tuning</span>
</span><span id="NetworkSlimming.prune-311"><a href="#NetworkSlimming.prune-311"><span class="linenos">311</span></a>
</span><span id="NetworkSlimming.prune-312"><a href="#NetworkSlimming.prune-312"><span class="linenos">312</span></a>        <span class="n">pytorch_converter</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">PyTorchConverter</span><span class="p">()</span>
</span><span id="NetworkSlimming.prune-313"><a href="#NetworkSlimming.prune-313"><span class="linenos">313</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="n">pytorch_converter</span><span class="o">.</span><span class="n">from_neural_network</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</span><span id="NetworkSlimming.prune-314"><a href="#NetworkSlimming.prune-314"><span class="linenos">314</span></a>
</span><span id="NetworkSlimming.prune-315"><a href="#NetworkSlimming.prune-315"><span class="linenos">315</span></a>        <span class="n">py_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pruning</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="NetworkSlimming.prune-316"><a href="#NetworkSlimming.prune-316"><span class="linenos">316</span></a>
</span><span id="NetworkSlimming.prune-317"><a href="#NetworkSlimming.prune-317"><span class="linenos">317</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</span><span id="NetworkSlimming.prune-318"><a href="#NetworkSlimming.prune-318"><span class="linenos">318</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">alt_rep_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">py_net</span><span class="p">)</span>
</span><span id="NetworkSlimming.prune-319"><a href="#NetworkSlimming.prune-319"><span class="linenos">319</span></a>        <span class="n">network</span><span class="o">.</span><span class="n">up_to_date</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="NetworkSlimming.prune-320"><a href="#NetworkSlimming.prune-320"><span class="linenos">320</span></a>
</span><span id="NetworkSlimming.prune-321"><a href="#NetworkSlimming.prune-321"><span class="linenos">321</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">network_transform</span><span class="o">.</span><span class="n">fine_tuning</span><span class="p">:</span>
</span><span id="NetworkSlimming.prune-322"><a href="#NetworkSlimming.prune-322"><span class="linenos">322</span></a>            <span class="n">network</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_strategy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</span><span id="NetworkSlimming.prune-323"><a href="#NetworkSlimming.prune-323"><span class="linenos">323</span></a>
</span><span id="NetworkSlimming.prune-324"><a href="#NetworkSlimming.prune-324"><span class="linenos">324</span></a>        <span class="k">return</span> <span class="n">network</span>
</span></pre></div>


            <div class="docstring"><p>Prune the neural network of interest using the pruning strategy Network Slimming.</p>

<h2 id="parameters">Parameters</h2>

<p>network : NeuralNetwork
    The neural network to prune.
dataset: Dataset
    The dataset to use for the pre-training and fine-tuning procedure.</p>

<h2 id="returns">Returns</h2>

<p>NeuralNetwork
    The Neural Network resulting from the application of the pruning strategy to the original network.</p>
</div>


                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>